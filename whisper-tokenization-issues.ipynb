{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fb3b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor, Seq2SeqTrainingArguments, WhisperForConditionalGeneration, Seq2SeqTrainer\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c906ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"whisper-large-finetuned_experiment-1_checkpoint-2000_fleurs-test.jsonl\", \"r\") as f:\n",
    "        with open(\"whisper-large-finetuned_experiment-1_checkpoint-2000_fleurs-hausa-test.jsonl\", \"w\") as out:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                json.dump(obj[0], out)\n",
    "                out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5e885db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fleurs (/data/users/kashrest/asr-experiments/google___fleurs/ha_ng/2.0.0/af82dbec419a815084fa63ebd5d5a9f24a6e9acdf9887b9e3b8c6bbd64e0b7ac)\n"
     ]
    }
   ],
   "source": [
    "cache_dir=\"/data/users/kashrest/asr-experiments\"\n",
    "data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"test\", cache_dir=cache_dir)\n",
    "labels = []\n",
    "with open(\"fleurs_hausa_test.jsonl\", \"w\") as f:\n",
    "    for elem in (iter(data)):\n",
    "        transcript = elem[\"transcription\"]\n",
    "        json.dump(transcript, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a8fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_igbo = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"igbo\", task=\"transcribe\")\n",
    "\n",
    "tokenizer_igbo.decode(tokenizer_igbo.encode(\"hello\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399c94ba",
   "metadata": {},
   "source": [
    "# Hausa "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b0fd3",
   "metadata": {},
   "source": [
    "The Whisper tokenizer has 169 characters in its vocabulary from the data of\"96\" non-English languages it was pretrained on: https://cdn.openai.com/papers/whisper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1edcdec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', '×', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'ā', 'ă', 'ą', 'ć', 'ĉ', 'ċ', 'č', 'ď', 'đ', 'ē', 'ĕ', 'ė', 'ę', 'ě', 'ĝ', 'ğ', 'ġ', 'ģ', 'ĥ', 'ħ', 'ĩ', 'ī', 'ĭ', 'į', 'ı', 'ĳ', 'ĵ', 'ķ', 'ĸ', 'ĺ', 'ļ', 'ľ', 'ŀ', 'ł', 'ń', '̇']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'from collections import Counter\\nchar_vocab_whisper = Counter(list(char_vocab_whisper))\\nprint(f\"In Whisper, the tokenizer vocabulary has {len(char_vocab_whisper.keys())} characters.\\n{char_vocab_whisper.most_common()}\")'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_hausa = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Hausa\", task=\"transcribe\")\n",
    "char_vocab_whisper = \"\"\n",
    "for key in tokenizer_hausa.get_vocab().keys():\n",
    "    char_vocab_whisper += \" \".join(key.lower())\n",
    "\n",
    "print(sorted(set(char_vocab_whisper)))\n",
    "\"\"\"from collections import Counter\n",
    "char_vocab_whisper = Counter(list(char_vocab_whisper))\n",
    "print(f\"In Whisper, the tokenizer vocabulary has {len(char_vocab_whisper.keys())} characters.\\n{char_vocab_whisper.most_common()}\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c967ff",
   "metadata": {},
   "source": [
    "For Hausa, setting the language makes sure the input to the model is prefixed by the language prefix (i.e. \"ha\"), and setting the task adds the task prefix (i.e. \"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a56c3948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftranscript|><|ha|><|transcribe|><|notimestamps|>Haƙiƙa bincikenka zai haifar da ɗa mai ido.<|endoftext|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_hausa.decode(tokenizer_hausa.encode(\"Haƙiƙa bincikenka zai haifar da ɗa mai ido.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d59bc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_hausa_vocab = [\"ɓ\", \"ƙ\", \"ɗ\", \"ƴ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54987196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ha ƙ i ƙ a   bin ci ken ka   z ai   ha if ar   da   ɗ a   m ai   ido .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_hausa.bpe(\"Haƙiƙa bincikenka zai haifar da ɗa mai ido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12693a40",
   "metadata": {},
   "source": [
    "Hausa special characters are not included when tokenizing, for some reason, but is included when using bpe.\n",
    "\n",
    "In this article: https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt, it says that GPT 2 does byte-level BPE, which encodes the byte values, so all possible characters will be acounted for. So which one is used by the model?\n",
    "\n",
    "Seeing as how the encode->decode returns the correct, original, text containing the special Hausa characters, I think the bpe is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d496bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer_hausa.tokenize(\"Haƙiƙa bincikenka zai haifar da ɗa mai ido.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3124f344",
   "metadata": {},
   "source": [
    "\"Ġ\" is space in byte-level BPE gpt2 tokenizer (which is the same tokenizer for Whisper for English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a437729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hausa.bpe(\"ƙaleen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56584bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hausa.tokenize(\"ƙaleen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b063c5",
   "metadata": {},
   "source": [
    "Make sure normalization is done with basic normalizer: https://github.com/huggingface/transformers/issues/20703\n",
    "\n",
    "This might be an issue later for languages like Yoruba. Edit: Might not be an issue? It looks like by default, text is not normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94506728",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hausa._normalize(\"Ça va?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4e4b0",
   "metadata": {},
   "source": [
    "Tokenizer vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ddb96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Whisper tokenizer vocab has {len(tokenizer_hausa.get_vocab().keys())} tokens\\n{tokenizer_hausa.get_vocab()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hausa.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf8c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_hausa_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d846b3",
   "metadata": {},
   "source": [
    "Look into adding new tokens (and possibly finetuning the model to learn contextual embeddings for these tokens?): https://medium.com/@pierre_guillou/nlp-how-to-add-a-domain-specific-vocabulary-new-tokens-to-a-subword-tokenizer-already-trained-33ab15613a41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e19edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hausa.bpe(\"Ina zama a wani ƙaramin ƙauye kilo mita hamsin tsakaninsu da birni.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4fd84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer_hausa.tokenize(\"Ina zama a wani ƙaramin ƙauye kilo mita hamsin tsakaninsu da birni.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d978a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hausa.bpe('Æ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c37719",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hausa.tokenize('Æ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hausa.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b490f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Æ'.encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec37088",
   "metadata": {},
   "outputs": [],
   "source": [
    "'ƙ'.encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hausa.add_tokens(tokenizer_hausa.bpe(\"Ina zama a wani ƙaramin ƙauye kilo mita hamsin tsakaninsu da birni.\").split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hausa.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hausa.tokenize(\"Ina zama a wani ƙaramin ƙauye kilo mita hamsin tsakaninsu da birni.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed893723",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hausa.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae24b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hausa.get_vocab()[\"ƙ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d41ae92",
   "metadata": {},
   "source": [
    "# Trying to understand tokenizer (byte-level BPE) done in Whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd743711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n",
    "    characters the bpe code barfs on.\n",
    "\n",
    "    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab\n",
    "    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for\n",
    "    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup\n",
    "    tables between utf-8 bytes and unicode strings.\n",
    "    \"\"\"\n",
    "    bs = (\n",
    "        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    )\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "def _tokenize(text):\n",
    "    \"\"\"Tokenize a string.\"\"\"\n",
    "    bpe_tokens = []\n",
    "    for token in re.findall(tokenizer_hausa.pat, text):\n",
    "        token = \"\".join(\n",
    "            bytes_to_unicode[b] for b in token.encode(\"utf-8\")\n",
    "        )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n",
    "        bpe_tokens.extend(bpe_token for bpe_token in tokenizer_hausa.bpe(token).split(\" \"))\n",
    "    return bpe_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f4d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hausa.pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf7f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenize(\"Ina zama a wani ƙaramin ƙauye kilo mita hamsin tsakaninsu da birni.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbbe5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hausa.tokenize(\"ƙ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('ƙ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9071fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(ord(\"!\"), ord(\"~\") + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c26120",
   "metadata": {},
   "outputs": [],
   "source": [
    "'ƙ'.encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92edfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bytes_to_unicode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b0428",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_to_unicode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b2a95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

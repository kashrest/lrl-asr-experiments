{"cells":[{"cell_type":"markdown","metadata":{"id":"a4ea45ff"},"source":["# Automatic Speech Recognition (ASR) Tutorial"],"id":"a4ea45ff"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":441,"status":"ok","timestamp":1690481483920,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"},"user_tz":420},"id":"EvUl_024YyLe","outputId":"88935e81-6046-4e85-bef8-9acff9a4eb12"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jul 27 18:11:24 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    42W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"],"id":"EvUl_024YyLe"},{"cell_type":"markdown","metadata":{"id":"691b82bb"},"source":["## Fine-tune a pretrained, multilingual ASR model on FLEURS"],"id":"691b82bb"},{"cell_type":"markdown","metadata":{"id":"9e0545f5"},"source":["In this tutorial, we will be evaluating and improving a multilingual ASR model for a language in the FLEURS dataset. We will focus on **Hausa**, but you can follow along in any language in Common Voice. See the [paper](https://arxiv.org/abs/2205.12446) for list of supported languages.\n","\n","We will be looking at three major open-source ASR multilingual models:\n","* XLS-R: [[paper]](https://arxiv.org/abs/2111.09296) [[Hugging Face blog]](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)\n","* Whisper: [[paper]](https://cdn.openai.com/papers/whisper.pdf) [[Hugging Face blog]](https://huggingface.co/blog/fine-tune-whisper#prepare-feature-extractor-tokenizer-and-data)\n","* MMS: [[paper]](https://scontent-sjc3-1.xx.fbcdn.net/v/t39.8562-6/348827959_6967534189927933_6819186233244071998_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=-JOSFMsFL-UAX-4O6o4&_nc_ht=scontent-sjc3-1.xx&oh=00_AfDdMFq0DP2xIRyjWpGrmIpqncnouiylLfWnFsAgxboLWw&oe=6497E242) [[Hugging Face blog]](https://huggingface.co/blog/mms_adapters)\n","\n","For more details on the models and finetuning them, please refer to the corresponding Hugging Face tutorials. Much of this tutorial draws from the Hugging Face blogs."],"id":"9e0545f5"},{"cell_type":"markdown","metadata":{"id":"6945531e"},"source":["## Before you start: Setting up your coding environment"],"id":"6945531e"},{"cell_type":"markdown","metadata":{"id":"89b7e5c2"},"source":["Make sure you follow the set up instructions in the [lrl-asr-experiments README](https://github.com/kashrest/lrl-asr-experiments) for this tutorial to run on Google Colab.\n","\n","\n","**Note**: The pretrained multilingual ASR models we will be using in this notebook require GPUs with at least 40 GB of space for practical use. If you are using Google Colab Pro, make sure you go to \"Runtime\" -> \"Change runtime type\" -> \"Hardware accelerator\" -> GPU and \"GPU type\" -> A100. You then should be able to run all lines of this tutorial."],"id":"89b7e5c2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"VW3g3xXx-jra"},"outputs":[],"source":["%%capture\n","!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","!pip install datasets[audio]\n","!pip install evaluate\n","!pip install git+https://github.com/huggingface/transformers.git\n","!pip install jiwer\n","!pip install accelerate -U"],"id":"VW3g3xXx-jra"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16721,"status":"ok","timestamp":1689948053058,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"},"user_tz":420},"id":"JIi7g9bvgMOl","outputId":"73b52e31-3c4a-4aa1-a83f-c8b2668e0f85"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# If you want to save checkpoints on your Google Drive (note: checkpoints\n","# may take up a few GBs (as explained in the README), so it is recommended that you download the checkpoints to your local machine instead), uncomment and run the lines below\n","\"\"\"from google.colab import drive\n","drive.mount('/content/drive')\"\"\""],"id":"JIi7g9bvgMOl"},{"cell_type":"markdown","metadata":{"id":"db71af9f"},"source":["## Data Preprocessing"],"id":"db71af9f"},{"cell_type":"markdown","metadata":{"id":"13dbbef6"},"source":["The first step is to download and prepare the data for the ASR model. Hugging Face has an easy way to download FLEURS data for any supported language, where the split can be specified. We also want to specify an output directory where our finetuned model checkpoints will live.\n","\n","**Note**: Make sure to download checkpoints to your local machine you want to investigate after checkpoint is saved because all data will be gone once runtime is terminated!"],"id":"13dbbef6"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24889296","outputId":"7a843c84-ba25-4825-815c-f007a907d3ec","executionInfo":{"status":"ok","timestamp":1690481537556,"user_tz":420,"elapsed":8737,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Output directory already exists; make a new directory.\n"]}],"source":["import os\n","from datasets import load_dataset\n","\n","# create a directory for outputs in tutorial\n","out_dir = \"./tutorial-fleurs/\" # NOTE: Since this is a directory in your virtual\n","                               # machine (which you can see in the side bar, under the folder icon),\n","                               # make sure to download model checkpoints to your\n","                               # local machine if you would like to investigate later\n","try:\n","    os.mkdir(out_dir)\n","except:\n","    print(\"Output directory already exists; make a new directory.\")\n","\n","# for Hausa, the language code is \"ha_ng\"\n","train_data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"train\")\n","val_data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"validation\")\n","test_data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"test\")\n"],"id":"24889296"},{"cell_type":"markdown","metadata":{"id":"8752ed3a"},"source":["FLEURS data is organized like so"],"id":"8752ed3a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ab66092","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690481538503,"user_tz":420,"elapsed":953,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"}},"outputId":"ffcd9be3-8df6-407b-8284-3f8a299104a9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': 302,\n"," 'num_samples': 301440,\n"," 'path': '/root/.cache/huggingface/datasets/downloads/extracted/6d52769d3af80da90bb14f9334ba8da9db2bc4cd9ccfdb87c34409eb360029ef/10002175198254707815.wav',\n"," 'audio': {'path': 'train/10002175198254707815.wav',\n","  'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n","         6.78300858e-05, 1.26361847e-05, 6.46114349e-05]),\n","  'sampling_rate': 16000},\n"," 'transcription': 'nasarorin da vautier ta samu marasa alaka da bada umarni ba sun hada da yajin cin abinci a 1973 a kan abin da ya ke ganin dabaibayin siyasa ne',\n"," 'raw_transcription': 'Nasarorin da Vautier ta samu marasa alaka da bada umarni ba sun hada da yajin cin abinci a 1973 a kan abin da ya ke ganin dabaibayin siyasa ne.',\n"," 'gender': 1,\n"," 'lang_id': 30,\n"," 'language': 'Hausa',\n"," 'lang_group_id': 3}"]},"metadata":{},"execution_count":4}],"source":["train_data[0]"],"id":"5ab66092"},{"cell_type":"markdown","metadata":{"id":"1c80129e"},"source":["and we have the training, validation, and test split with 1926, 580, 659, examples respectively."],"id":"1c80129e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"12a6108f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690481538504,"user_tz":420,"elapsed":9,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"}},"outputId":"8985670d-fafb-490a-f808-f2ef5243b5d6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3259, 296, 621)"]},"metadata":{},"execution_count":5}],"source":["len(train_data), len(val_data), len(test_data)"],"id":"12a6108f"},{"cell_type":"markdown","metadata":{"id":"ec16d08d"},"source":["We are interested in the audio ([represented as an array of floats each proportional to the intensity of the sound at a certain point in time](http://artsites.ucsc.edu/EMS/Music/tech_background/TE-16/teces_16.html); the number of floats is determined by the sampling rate which 16,000 Hz, or 16,000 measurements per second) and the corresponding transcript.\n","\n","**Note**: All three models we will be using in this tutorial require that the data is sampled at 16,000 Hz. Since FLEURS is sampled at 16,000 Hz, we are good."],"id":"ec16d08d"},{"cell_type":"markdown","metadata":{"id":"ca52aa2e"},"source":["Let's extract the audio and transcripts"],"id":"ca52aa2e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6d6e448"},"outputs":[],"source":["train_transcripts, val_transcripts, test_transcripts = [], [], []\n","train_audio, val_audio, test_audio = [], [], []\n","\n","for elem in train_data:\n","    assert elem[\"audio\"][\"sampling_rate\"] == 16000\n","    train_audio.append(elem[\"audio\"][\"array\"])\n","    train_transcripts.append(elem[\"raw_transcription\"])\n","\n","for elem in val_data:\n","    assert elem[\"audio\"][\"sampling_rate\"] == 16000\n","    val_audio.append(elem[\"audio\"][\"array\"])\n","    val_transcripts.append(elem[\"raw_transcription\"])\n","\n","for elem in test_data:\n","    assert elem[\"audio\"][\"sampling_rate\"] == 16000\n","    test_audio.append(elem[\"audio\"][\"array\"])\n","    test_transcripts.append(elem[\"raw_transcription\"])"],"id":"f6d6e448"},{"cell_type":"markdown","metadata":{"id":"9945c231"},"source":["Now, since we are interested in transcribing speech, we want to clean the transcripts by removing special characters that do not have a clear sound (such as ! '). This part may depend on your target application and language. For example for Hausa, many native speakers do not speak English and does not have much code-switching, so we also normalize any foreign characters (ç ş) and symbols (% & $)."],"id":"9945c231"},{"cell_type":"code","execution_count":null,"metadata":{"id":"12b34939"},"outputs":[],"source":["import re\n","\n","def preprocess_texts_hausa(transcriptions):\n","    chars_to_remove_regex = '[><¥£°¾½²\\\\\\+\\,\\?\\!\\-\\;\\:\\\"\\“\\%\\‘\\'\\ʻ\\”\\�\\$\\&\\(\\)\\–\\—\\[\\]\\{\\}/]'\n","\n","    def _remove_special_characters(transcription):\n","        transcription = transcription.strip() # remove any leading or trailing white space\n","        transcription = transcription.lower()\n","        transcription = re.sub(chars_to_remove_regex, '', transcription)\n","        return transcription\n","\n","    def _normalize_diacritics(transcription):\n","        a = '[āăáã]'\n","        u = '[ūúü]'\n","        o = '[öõó]'\n","        c = '[ç]'\n","        i = '[í]'\n","        s = '[ş]'\n","        e = '[é]'\n","\n","        transcription = re.sub(a, \"a\", transcription)\n","        transcription = re.sub(u, \"u\", transcription)\n","        transcription = re.sub(o, \"o\", transcription)\n","        transcription = re.sub(c, \"c\", transcription)\n","        transcription = re.sub(i, \"i\", transcription)\n","        transcription = re.sub(s, \"s\", transcription)\n","        transcription = re.sub(e, \"e\", transcription)\n","\n","        return transcription\n","\n","    cleaned_transcriptions = map(_remove_special_characters, transcriptions)\n","    cleaned_transcriptions = list(map(_normalize_diacritics, list(cleaned_transcriptions)))\n","    return cleaned_transcriptions\n","\n","train_transcripts = preprocess_texts_hausa(train_transcripts)\n","val_transcripts = preprocess_texts_hausa(val_transcripts)\n","test_transcripts = preprocess_texts_hausa(test_transcripts)"],"id":"12b34939"},{"cell_type":"markdown","metadata":{"id":"2e40e306"},"source":["**Note**: It is important to preprocess test transcripts the same way as the training transcripts so that we have a fair evaluation of the model."],"id":"2e40e306"},{"cell_type":"markdown","metadata":{"id":"d341925a"},"source":["Some models (MMS and XLS-R) predict one character at a time, and so we need a character vocabulary made up of all characters in the dataset after preprocessing. We can save the vocabulary in a JSON file"],"id":"d341925a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9f34513e"},"outputs":[],"source":["import json\n","\n","def extract_all_chars(transcription):\n","      all_text = \" \".join(transcription)\n","      vocab = list(set(all_text))\n","      return {\"vocab\": [vocab], \"all_text\": [all_text]}\n","\n","vocab_train = list(map(extract_all_chars, train_transcripts))\n","vocab_val = list(map(extract_all_chars, val_transcripts))\n","vocab_test = list(map(extract_all_chars, test_transcripts))\n","\n","vocab_train_chars = []\n","for elem in [elem[\"vocab\"][0] for elem in vocab_train]:\n","    vocab_train_chars.extend(elem)\n","\n","vocab_val_chars = []\n","for elem in [elem[\"vocab\"][0] for elem in vocab_val]:\n","    vocab_val_chars.extend(elem)\n","\n","vocab_test_chars = []\n","for elem in [elem[\"vocab\"][0] for elem in vocab_test]:\n","    vocab_test_chars.extend(elem)\n","\n","vocab_list = list(set(vocab_train_chars) | set(vocab_val_chars) | set(vocab_test_chars))\n","vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n","\n","# for word delimiter, change \" \" --> \"|\" (ex. \"Hello my name is Bob\" --> \"Hello|my|name|is|Bob\")\n","vocab_dict[\"|\"] = vocab_dict[\" \"]\n","del vocab_dict[\" \"]\n","vocab_dict[\"[UNK]\"] = len(vocab_dict)\n","vocab_dict[\"[PAD]\"] = len(vocab_dict) # this is for models (like MMS and XLS-R) that use the CTC algorithm to predict the end of a character (e.g. \"hhh[PAD]iii[PAD]iii[PAD]\" == \"hii\")"],"id":"9f34513e"},{"cell_type":"markdown","metadata":{"id":"9dca95bc"},"source":["This is the character vocabulary based on the FLEURS dataset"],"id":"9dca95bc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1265346a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690481557805,"user_tz":420,"elapsed":24,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"}},"outputId":"d0064714-6b41-4554-e627-5c4dec590184"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'j': 0,\n"," '6': 1,\n"," 'h': 2,\n"," 'v': 3,\n"," 'x': 4,\n"," 'c': 5,\n"," 'g': 6,\n"," 'ɗ': 7,\n"," 'q': 8,\n"," 'y': 9,\n"," 'r': 10,\n"," '1': 11,\n"," 'o': 12,\n"," 's': 13,\n"," 'u': 14,\n"," '.': 16,\n"," 'ƙ': 17,\n"," 'b': 18,\n"," 'p': 19,\n"," 'a': 20,\n"," '5': 21,\n"," '4': 22,\n"," 'n': 23,\n"," 't': 24,\n"," 'f': 25,\n"," 'ƴ': 26,\n"," 'l': 27,\n"," 'e': 28,\n"," '7': 29,\n"," '3': 30,\n"," '8': 31,\n"," 'z': 32,\n"," 'm': 33,\n"," 'i': 34,\n"," '0': 35,\n"," 'k': 36,\n"," '9': 37,\n"," '2': 38,\n"," 'ɓ': 39,\n"," 'w': 40,\n"," '’': 41,\n"," 'd': 42,\n"," '|': 15,\n"," '[UNK]': 43,\n"," '[PAD]': 44}"]},"metadata":{},"execution_count":9}],"source":["vocab_dict"],"id":"1265346a"},{"cell_type":"markdown","metadata":{"id":"23006af1"},"source":["Let's save this vocabulary file for later use in the output folder."],"id":"23006af1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1880811"},"outputs":[],"source":["vocab_file = out_dir+\"vocab_hausa.json\"\n","with open(vocab_file, 'w') as f:\n","    json.dump(vocab_dict, f)"],"id":"c1880811"},{"cell_type":"markdown","metadata":{"id":"74417458"},"source":["## Evaluation code"],"id":"74417458"},{"cell_type":"markdown","metadata":{"id":"c7b7c289"},"source":["In ASR, word error rate [(WER)](https://huggingface.co/spaces/evaluate-metric/wer) and character error rate [(CER)](https://huggingface.co/spaces/evaluate-metric/cer) are the common metrics used to evaluate how good a model-produced transcript is in comparison to the gold transcript. These metrics are related to the \"edit distance\" between two strings and offer a quantitative measure of string difference.\n","\n","Let's create a simple function that takes in two sets of strings and calculates the WER and CER of the predicted strings over the dataset."],"id":"c7b7c289"},{"cell_type":"code","execution_count":null,"metadata":{"id":"117f2156"},"outputs":[],"source":["from datasets import load_dataset, Audio\n","import evaluate\n","\n","def compute_metrics(label_strs, pred_strs):\n","    wer_metric = evaluate.load(\"wer\")\n","    cer_metric = evaluate.load(\"cer\")\n","\n","    wer = wer_metric.compute(predictions=pred_strs, references=label_strs) * 100\n","    cer = cer_metric.compute(predictions=pred_strs, references=label_strs) * 100\n","    return {\"wer\": wer, \"cer\": cer}"],"id":"117f2156"},{"cell_type":"markdown","metadata":{"id":"8c88b179"},"source":["## Section A: Zero-Shot ASR"],"id":"8c88b179"},{"cell_type":"markdown","metadata":{"id":"4ebc9422"},"source":["Let's run inference on our dataset with Whisper and MMS-1b-all, which are models that are usable off-the-shelf. We will determine performance on the test set since some models will be later fine-tuned on the train split."],"id":"4ebc9422"},{"cell_type":"markdown","metadata":{"id":"3cec099d"},"source":["### OpenAI Whisper"],"id":"3cec099d"},{"cell_type":"markdown","metadata":{"id":"5a72baf2"},"source":["OpenAI's Whisper model is a pretrained encoder-decoder model that supports a set of languages without futher fine-tuning. Here, we will use whisper-medium. You can use the larger checkpoints if you have enough GPU memory (found on Hugging Face Hub: https://huggingface.co/openai/whisper-medium).\n","\n","Note: Whisper requires that input is sampled at 16,000 Hz. Also, Whisper may not support all FLEURS languages, so make sure to check the [paper](https://cdn.openai.com/papers/whisper.pdf)."],"id":"5a72baf2"},{"cell_type":"markdown","metadata":{"id":"fb0e2665"},"source":["With a batch size of 10, inference takes about 15 minutes."],"id":"fb0e2665"},{"cell_type":"code","execution_count":null,"metadata":{"id":"23266ffc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690482456931,"user_tz":420,"elapsed":894169,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"}},"outputId":"ec310c73-7989-4239-cea7-23f8dc0e7073"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/63 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1396: UserWarning: Using the model-agnostic default `max_length` (=448) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","100%|██████████| 63/63 [14:42<00:00, 14.00s/it]\n"]}],"source":["from transformers import WhisperProcessor, WhisperForConditionalGeneration\n","import transformers\n","from tqdm import tqdm\n","import torch\n","\n","device = \"cuda:0\" # change this to a custom gpu if you have access to one, otherwise set to \"cpu\"\n","model_id = \"openai/whisper-medium\"\n","processor = WhisperProcessor.from_pretrained(model_id)\n","model = WhisperForConditionalGeneration.from_pretrained(model_id).to(device)\n","forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"hausa\", task=\"transcribe\")\n","\n","predicted_test_transcripts = []\n","\n","batch_size = 10 # decrease if needed\n","\n","for i in tqdm(range(0, len(test_audio), batch_size)):\n","    batch = test_audio[i:i+batch_size] if i+batch_size <= len(test_audio) else test_audio[i:]\n","    input_features = processor(batch, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n","    # generate token ids\n","    predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n","    # decode token ids to text\n","    predicted_test_transcripts.extend(processor.batch_decode(predicted_ids, skip_special_tokens=True))\n","    # free GPU memory for upcoming models\n","    del input_features\n","    torch.cuda.empty_cache()\n","\n","# free GPU memory for upcoming models\n","del model\n","torch.cuda.empty_cache()"],"id":"23266ffc"},{"cell_type":"markdown","metadata":{"id":"73753c9b"},"source":["Let's evaluate the performance of whisper-medium on our preprocessed test dataset"],"id":"73753c9b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"521fe204","colab":{"base_uri":"https://localhost:8080/","height":98,"referenced_widgets":["b357618388cf4e09be5cbfbfe3b6d4da","d17999c9b3d744f3a038879aafdbea13","71a342a18ddc4051b22b70284fbb5384","6980111d4d704f2c8df3bff2fe3207f6","d55196e986084aef9ba0109ef3d878a5","8947606c562d498fbca60413c2d8f9e9","940b1180514343b1990727a434f2b345","5b5ec84a8a9948f5aae868af7d2503eb","8007b98d91344fcaa6ec56514e85b96e","248e1e917db648c7822f6da61329deae","f760b0e03c74488a83c3a217ebf33bc7","7197a0f55cd04775974d89d1c0cb17ec","bc21e88298f648eaad07a42ff1de4f28","c3de9ff702304d7bad85d35159a2b13e","3457432e869044e4a51ab9c278b7a9ad","c20d520690e94688b9e40ddc56faa868","d641c697e39d4d95b9106f907eeef87b","3f2ba54ed1244fd6bc6905e822e2ea2a","6dd17e1ccda04467a238ea4891824fd1","33be0b8885594343be102192f2fb38de","5aefdd3d6bc8408a81abeebc70b0192f","8ab5d2901f5b4e369f56c8e08e2c8e50"]},"executionInfo":{"status":"ok","timestamp":1690482460261,"user_tz":420,"elapsed":3360,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"}},"outputId":"74319b07-4bcb-4778-a75b-ab3d237543a5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b357618388cf4e09be5cbfbfe3b6d4da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/5.60k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7197a0f55cd04775974d89d1c0cb17ec"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'wer': 129.91414659149154, 'cer': 75.46085873151374}"]},"metadata":{},"execution_count":13}],"source":["compute_metrics(test_transcripts, predicted_test_transcripts)"],"id":"521fe204"},{"cell_type":"markdown","metadata":{"id":"8dbe8e1a"},"source":["It looks like we have a 129.9% WER and 75.5% CER. Let's create a running table of the performances of different models on our test dataset.\n","\n","| Model | WER % | CER %|\n","|-------|-----|----|\n","|whisper-medium|129.9|75.5|"],"id":"8dbe8e1a"},{"cell_type":"markdown","metadata":{"id":"ee05534e"},"source":["This is very poor performance, as WER is 129.9%, meaning all words in the dataset were incorrect, and the model predicted more words than are present in the reference text. CER is also very poor with 75.5%, meaning the model predicted incorrect characters on average three quarters of the time. A random example prediction is shown below"],"id":"ee05534e"},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":424,"status":"ok","timestamp":1690482517503,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"},"user_tz":420},"id":"2c95ce82","colab":{"base_uri":"https://localhost:8080/"},"outputId":"54487a3d-5e3d-443f-87db-c1cdd1e51717"},"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted transcript:  Tigeak kyanqashi ishwin chikin tari nirwanda ke zuwa da ga koraamun duenia, zuha chikin te huyana azwani da ga amu zul.\n","Reference transcript: cikakken kashi 20 cikin dari na ruwan da ke zubowa daga koramun duniyar zuwa cikin teku yana zuwa ne daga amazon.\n"]}],"source":["import random\n","n = random.randint(0, len(predicted_test_transcripts)-1)\n","print(f\"Predicted transcript: {predicted_test_transcripts[n]}\\nReference transcript: {test_transcripts[n]}\")"],"id":"2c95ce82"},{"cell_type":"markdown","metadata":{"id":"4906ec93"},"source":["Since Whisper has not been fine-tuned on our dataset, foreign characters and capitalization seems to contribute to the CER/WER. We will later see if we can improve the scores with finetuning.\n","\n","**Note**: Manual error analysis is important to do along with looking at WER and CER. Sometimes, although the WER/CER is poor, the transcripts are not completely inaccurate, as you may see above.\n","\n","**Note**: Also, Whisper only predicts for up to 30 secs of audio, so if you have longer samples, you will get poor WER/CER"],"id":"4906ec93"},{"cell_type":"markdown","metadata":{"id":"e6a2a508"},"source":["### Facebook MMS"],"id":"e6a2a508"},{"cell_type":"markdown","metadata":{"id":"4289c3f0"},"source":["MMS-1b-all is Facebook's MMS (**M**assively **M**ultilingual **S**peech) model, which is MMS, a Wav2Vec model that is pretrained on a large corpus of Bible data covering 1107 languages, and finetuned on additional labeled datasets. MMS is pretrained similarly to how BERT is trained with a masked language modeling objective, but by masking audio input. We will use MMS-1b-all to run inference on our dataset."],"id":"4289c3f0"},{"cell_type":"markdown","metadata":{"id":"7b7909c2"},"source":["A batch size of 10 takes approximately 3 minutes."],"id":"7b7909c2"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258,"referenced_widgets":["64c6ce7f160c47b491bc6a20fec1fe3f","665677b287dc43518acf87289411e908","ab10e506999e45fe8a02c0a9d5b555c4","8bd7f57208394a50a392e6efe5b1a17d","b45eb56065ea42e880489738d504fd8b","9dd31774a97c403e905bf0ee2f5580ac","5032523644de473e94d8a97aa7c8b7c9","93fca2dc97974703b6539e6af75bf602","d0eaa0bfe627499ea1c22f255d5adea4","5a25418810674f7093c888d6446d1600","6cdf364020204ad4848353c79f145455","2f9812efe93a410a8b6ff0b703ed9107","448269ead4104085a1c0a8e59e32a9a3","0828860a87b7474a882ca3f91ccb1dd9","348a9c3d036446c6b936cb73f61e7c6c","94974fbfaaa8460a8ef39bda41ddf61c","3a42fe367dfe44e393d7a70eee720e06","30bbf28fcbcb4e5685cfc7ec407ca66f","32b093ba8c6f4af498b2657123467248","f8a31cfbab934a6ca363841830d4c80c","c583d252107c4ea587f7af8afdc9c5d9","8f21ba95fd944c2581d382778ab316ab","f620f13c1c844c6f86a489dc9fa139a7","862ef0d78f074a678a76140d892292e2","45a9410445204c27a9981aba02e510ab","dda44a36994e4ef69fb50b146cbe3c61","80f290e382434287b92c15d9c6720213","30b9afafb04e4398b0e1358c8c8b9746","1f735c9c15ac45ee978890521dc4f44e","ee6c870a2b3243b8b37cbe1576a27448","824ba2b5e8204973b03e4110fe1d7141","5f2bd9667b2f40f79562753e78074a0f","8ebc59bc7b3e4130ab76a270ec1ebb47","288b1480250148289318e1ba88ad6093","d7638dd1877244318525421c622c7f10","2092da2ef1da4dcabd76628d2679b066","8652f78d68e3435dab1b9055add8b483","c6a7a258c8d546b3a54033f7e5e18e89","2dde7a6a3dd04f14968b3cfabec4724a","88cf7f3e4beb40dc92c04d83ee583aca","952a819f1ed440dbbdc2c92e45cd3036","7565f20585764151a73295223ea3b235","f3b60735d5804771be195dfef0ef573f","83de379564f541d49dd50741152ba8a0","4e6529c49c694bc18292df229f78a0d4","99f9fadfd82840ff9141335512b5aa1e","b392c970a92c4c9a823ea333fa462a75","f3057681a31249caa49b2d6ea8bdd7cb","b88667fbb8c643a381ecf5f3fbb24643","3111ee71fd4b4aa28930bbbd8206333a","0c549e2fa8354f4da4a550af97e8440c","b7eda0d07b47401f92b3a283fe6eb0c4","8b2694e8de5c4d7282c7f39c76df52ef","75a458feb0af407eb8edaac3cef0f858","e4ac24638430410fb03b1cb09993780c","8710b75ce5d64c94822334668ef82e84","4b136d9fd4e640ea9f2a42c786537e32","8bdee04c4cee42ca80ad28589908e817","db6f7a46f0134f71a0c81e248f28a031","6c304820e972458ea0529b0348fe9f2f","834341f488d945738c3d9fd505bcff46","7950db7975c34d30b0288e7191e18307","09472eb97a7741f8a68253e23c940277","560e235f3e5545dc861745c6e9c69160","eec91ceb8f434d54bc92d73dfe2759b8","a1fadda70dd34f78be74f16ed64711e5","8ce9ba374992451da81860ff974e741f","8f11061d98fa4aa0b7098c82500beaac","50f4a21fe20d4ace820b5b991e8debce","0b8375de02e84e47a488461f6d780086","ee2d974deb6e4c59b4d5def3ac89e6df","ecd1e6f057964e62b4d696fbdfa5e1fa","58a9e63ee68d4a54b8a9938f90fe4f27","6e2bf246e6f4422289b9526b1ddf8e3c","44f32366ace3408ead280bec9c2a2b37","ecd824e7f0324eff91e4deea43c9d17c","992badfaab714f39897fbd5d2c93f049"]},"executionInfo":{"elapsed":230609,"status":"ok","timestamp":1690482843073,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"},"user_tz":420},"id":"7db55c0c","outputId":"3c9f241c-a309-44d0-eec7-3971f316ed0c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)rocessor_config.json:   0%|          | 0.00/254 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64c6ce7f160c47b491bc6a20fec1fe3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/397 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f9812efe93a410a8b6ff0b703ed9107"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f620f13c1c844c6f86a489dc9fa139a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"288b1480250148289318e1ba88ad6093"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/2.04k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e6529c49c694bc18292df229f78a0d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8710b75ce5d64c94822334668ef82e84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)pter.hau.safetensors:   0%|          | 0.00/9.07M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ce9ba374992451da81860ff974e741f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["100%|██████████| 63/63 [03:18<00:00,  3.16s/it]\n"]}],"source":["import torch\n","from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n","\n","model_id = \"facebook/mms-1b-all\"\n","\n","device = \"cuda:0\" # change this to a custom gpu if you have access to one, otherwise set to \"cpu\"\n","\n","processor = Wav2Vec2Processor.from_pretrained(model_id)\n","model = Wav2Vec2ForCTC.from_pretrained(model_id).to(device)\n","\n","processor.tokenizer.set_target_lang(\"hau\")\n","model.load_adapter(\"hau\")\n","\n","\n","predicted_test_transcripts = []\n","\n","batch_size = 10\n","\n","for i in tqdm(range(0, len(test_audio), batch_size)):\n","    batch = test_audio[i:i+batch_size] if i+batch_size <= len(test_audio) else test_audio[i:]\n","    inputs = processor(batch, sampling_rate=16_000, return_tensors=\"pt\", padding=True).to(device)\n","    with torch.no_grad():\n","        outputs = model(**inputs).logits\n","    # free GPU memory for upcoming models\n","    del inputs\n","    torch.cuda.empty_cache()\n","    ids = torch.argmax(outputs, dim=-1)\n","    predicted_test_transcripts.extend((processor.batch_decode(ids)))\n","\n","# free GPU memory for upcoming models\n","del model\n","torch.cuda.empty_cache()"],"id":"7db55c0c"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2890,"status":"ok","timestamp":1690483980107,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"},"user_tz":420},"id":"9868c9ce","outputId":"acf8cc43-48e3-4603-87cc-4f7af35ec063"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'wer': 29.286263454638643, 'cer': 7.7002053388090355}"]},"metadata":{},"execution_count":16}],"source":["compute_metrics(test_transcripts, predicted_test_transcripts)"],"id":"9868c9ce"},{"cell_type":"markdown","metadata":{"id":"3c4874ff"},"source":["It looks like we have a 29.3% WER and 7.7% CER. Let's add this result to our table. This is substantially better than whisper-medium! Although, if you have more GPU memory, it would be better to compare Whisper-large with MMS-1b-all since both have about 1 billion parameters.\n","\n","| Model | WER % | CER %|\n","|-------|-----|----|\n","|whisper-medium|129.9|75.5|\n","|mms-1b-all|29.3|7.7|"],"id":"3c4874ff"},{"cell_type":"markdown","metadata":{"id":"e9c10870"},"source":["Here's a random example. These are more accurate than Whisper."],"id":"e9c10870"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1239,"status":"ok","timestamp":1690483989547,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"},"user_tz":420},"id":"38fb86f7","outputId":"dd38d542-307a-46fc-be7f-151a46af2426"},"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted transcript: amma ana san ya ku a cikin manyan yankuna masu tsakatsakin kawai 'yan dingiri kadan arewacin kerjin za ku bukaci mu'ammala da zafin rana koyaushe da rana mai ƙarbi lokacin da sama ta bayyana mafi wuya\n","Reference transcript: amma ana sanya ku a cikin manyan yankuna masu tsakatsakin kawai yan digiri kaɗan arewacin kerjin za ku buƙaci maamala da zafin rana koyaushe da rana mai ƙarfi lokacin da sama ta bayyana mafi wuya.\n"]}],"source":["import random\n","n = random.randint(0, len(predicted_test_transcripts)-1)\n","print(f\"Predicted transcript: {predicted_test_transcripts[n]}\\nReference transcript: {test_transcripts[n]}\")"],"id":"38fb86f7"},{"cell_type":"markdown","metadata":{"id":"59a6325a"},"source":["## Section B: Finetuning"],"id":"59a6325a"},{"cell_type":"markdown","metadata":{"id":"3aacf6e2"},"source":["For fine-tuning, we will be using functions from the Hugging Face API for the training loop and model setup. In order to use these functions, we need to wrap the data in a custom PyTorch Dataset object. We have two types of models: Wav2Vec2 (XLS-R, MMS) and Seq2Seq (Whisper), with different processors, so we need two objects\n","\n","**Note**: For example purposes, we finetune the following models for 3 epochs. In reality, time permitting, it is better to finetune for more (10+) to see when the loss stabalizes."],"id":"3aacf6e2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4c1e5634"},"outputs":[],"source":["import torch\n","class ASRDatasetWav2Vec2(torch.utils.data.Dataset):\n","    def __init__(self, audio, transcripts, sampling_rate, processor):\n","        self.audio = audio\n","        self.transcripts = transcripts\n","        self.sampling_rate = sampling_rate\n","        self.processor = processor\n","\n","    def __getitem__(self, idx):\n","        input_values = self.processor.feature_extractor(self.audio[idx], sampling_rate=self.sampling_rate).input_values[0]\n","        labels = self.processor.tokenizer(self.transcripts[idx]).input_ids\n","        item = {}\n","        item[\"input_values\"] = input_values\n","        item[\"labels\"] = labels\n","\n","        return item\n","\n","    def __len__(self):\n","        return len(self.transcripts)"],"id":"4c1e5634"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9c30972"},"outputs":[],"source":["class ASRDatasetWhisper(torch.utils.data.Dataset):\n","    def __init__(self, audio, transcripts, sampling_rate, processor):\n","        self.audio = audio\n","        self.transcripts = transcripts\n","        self.sampling_rate = sampling_rate\n","        self.processor = processor\n","\n","    def __getitem__(self, idx):\n","        input_values = self.processor.feature_extractor(self.audio[idx], sampling_rate=self.sampling_rate).input_features[0]\n","        labels = self.processor.tokenizer(self.transcripts[idx]).input_ids\n","        item = {}\n","        item[\"input_features\"] = input_values\n","        item[\"labels\"] = labels\n","\n","        return item\n","\n","    def __len__(self):\n","        return len(self.transcripts)"],"id":"a9c30972"},{"cell_type":"markdown","metadata":{"id":"1358da3f"},"source":["### Whisper"],"id":"1358da3f"},{"cell_type":"markdown","metadata":{"id":"72b2a9a4"},"source":["First, let's import the required Whisper classes and training loop functions from Hugging Face and some other utility functions\n","\n","**Note**: Hugging Face has a great tutorial that we referenced for fine-tuning Whisper. You can refer to [this tutorial](https://huggingface.co/blog/fine-tune-whisper#prepare-feature-extractor-tokenizer-and-data) for more information if needed."],"id":"72b2a9a4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bd7b8cd9"},"outputs":[],"source":["from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Optional, Union\n","import transformers\n","transformers.set_seed(9)"],"id":"bd7b8cd9"},{"cell_type":"markdown","metadata":{"id":"ffca4305"},"source":["Let's fine-tune a Whisper-medium model which we will download from Hugging Face, and set up a WhisperProcessor object which contains a feature extractor and a tokenizer. The feature extractor transforms the input into log-Mel spectrograms. This transformation takes in the amplitude information respresented by the input array and transforms it into frequencies (refer to the Hugging Face tutorial for more information). Frequencies encode pitch, and so useful audio signals can be found for speech recognition. Additionally, the tokenizer splits the transcripts into tokens based on Whisper's vocabulary. Whisper utilizes byte-level BPE, which is the same tokenizer as GPT-2. If interested, refer to this page: https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt. This tokenizer enables encoding of any character."],"id":"ffca4305"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7cab8e81"},"outputs":[],"source":["model_card = \"openai/whisper-medium\"\n","processor = WhisperProcessor.from_pretrained(model_card, language=\"Hausa\", task=\"transcribe\")\n","model = WhisperForConditionalGeneration.from_pretrained(model_card)"],"id":"7cab8e81"},{"cell_type":"markdown","metadata":{"id":"92aec1c5"},"source":["The following lines are required to fine-tune the Whisper model. The first line makes the model predict the language and task by setting the token ids that control the transcription language and task, to `None`.\n","\n","The second line makes sure that all possible tokens are predicted by setting the set of supressed tokens to an empty list."],"id":"92aec1c5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7657fffc"},"outputs":[],"source":["model.config.forced_decoder_ids = None\n","model.config.suppress_tokens = []"],"id":"7657fffc"},{"cell_type":"markdown","metadata":{"id":"f20c83da"},"source":["Like mentioned before, Whisper takes inputs sampled at 16,000 Hz, and so we will prepare our data using this sampling rate using the ASRDataset object mentioned before"],"id":"f20c83da"},{"cell_type":"code","execution_count":null,"metadata":{"id":"806959ee"},"outputs":[],"source":["model_sampling_rate = 16000\n","train_dataset = ASRDatasetWhisper(train_audio, train_transcripts, model_sampling_rate, processor)\n","val_dataset = ASRDatasetWhisper(val_audio,  val_transcripts, model_sampling_rate, processor)\n","test_dataset = ASRDatasetWhisper(test_audio, test_transcripts, model_sampling_rate, processor)"],"id":"806959ee"},{"cell_type":"markdown","metadata":{"id":"bb3fab6f"},"source":["Next, we need a function that will pad all the inputs/outputs in a batch to the same length. This code is from the tutorial mentioned earlier."],"id":"bb3fab6f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1d1fdde6"},"outputs":[],"source":["@dataclass\n","class DataCollatorSpeechSeq2SeqWithPadding:\n","    processor: Any\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # split inputs and labels since they have to be of different lengths and need different padding methods\n","        # first treat the audio inputs by simply returning torch tensors\n","        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n","        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n","\n","        # get the tokenized label sequences\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","        # pad the labels to max length\n","        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n","\n","        # replace padding with -100 to ignore loss correctly\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        # if bos token is appended in previous tokenization step,\n","        # cut bos token here as it's append later anyways\n","        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n","            labels = labels[:, 1:]\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch\n","\n","data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"],"id":"1d1fdde6"},{"cell_type":"markdown","metadata":{"id":"24fab3d7"},"source":["In order to use the Trainer class from Hugging Face, we need to define an evaluation function that takes in a model prediction object."],"id":"24fab3d7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"52d2304f"},"outputs":[],"source":["import evaluate\n","def compute_metrics(pred):\n","    wer_metric = evaluate.load(\"wer\")\n","    cer_metric = evaluate.load(\"cer\")\n","\n","    pred_ids = pred.predictions\n","    label_ids = pred.label_ids\n","\n","    # replace -100 with the pad_token_id\n","    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n","\n","    # we do not want to group tokens when computing the metrics\n","    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","\n","    wer = 100 * wer_metric.compute(predictions=pred_str, references=label_str)\n","    cer = 100 * cer_metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"wer\": wer, \"cer\": cer}"],"id":"52d2304f"},{"cell_type":"markdown","metadata":{"id":"58b7c41f"},"source":["Now, we will setup the model training hyperparameters by using Hugging Face Seq2SeqTrainingArguments. Feel free to experiment with different hyperparameters. Learning rate is an important hyperparameter to experiment with. Reference the official Seq2SeqTrainingArguments for explanations of the hyperparameters: https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments\n","\n","Note: Decrease batch size if you have limited GPU space. We have also set the mixed precision"],"id":"58b7c41f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a77b5a0b"},"outputs":[],"source":["training_args = Seq2SeqTrainingArguments(\n","    output_dir=out_dir+\"whisper-finetuning-experiment-1/\",  # change to a repo name of your choice\n","    per_device_train_batch_size=16,\n","    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\n","    learning_rate=1e-05,\n","    warmup_steps=500,\n","    num_train_epochs=3,\n","    gradient_checkpointing=True, # another way to save GPU memory by recomputing gradients (less memory, more time)\n","    fp16=True, # this enables mixed precision training, which lets some data be stored in 16 bit floating point precision instead of 32 bits.\n","    evaluation_strategy=\"steps\",\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_steps=100,\n","    eval_steps=100,\n","    save_total_limit=2,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"wer\",\n","    greater_is_better=False)"],"id":"a77b5a0b"},{"cell_type":"markdown","metadata":{"id":"ada10041"},"source":["Now, we set up the Trainer object by inputing our training and validation datasets, our evaluation function, tokenizer, model, data collator, and previously instantiated training arguments."],"id":"ada10041"},{"cell_type":"code","execution_count":null,"metadata":{"id":"47ee3809","scrolled":true},"outputs":[],"source":["trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=processor.feature_extractor,\n",")"],"id":"47ee3809"},{"cell_type":"markdown","metadata":{"id":"e5847d83"},"source":["Then we call train to start training. Training the Whisper medium model with batch size 16 for 3 epochs takes about 40 minutes."],"id":"e5847d83"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"elapsed":2193423,"status":"ok","timestamp":1689955296700,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"},"user_tz":420},"id":"7e15828e","outputId":"ff37db00-1f25-4cc4-9d17-9dd2b7fc9bb5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [306/306 36:25, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Wer</th>\n","      <th>Cer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>No log</td>\n","      <td>1.681253</td>\n","      <td>77.807152</td>\n","      <td>35.971842</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>No log</td>\n","      <td>0.724689</td>\n","      <td>100.000000</td>\n","      <td>100.000000</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>No log</td>\n","      <td>0.606645</td>\n","      <td>99.693892</td>\n","      <td>99.615112</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=306, training_loss=1.445460400550194, metrics={'train_runtime': 2193.1387, 'train_samples_per_second': 4.458, 'train_steps_per_second': 0.14, 'total_flos': 9.97845418082304e+18, 'train_loss': 1.445460400550194, 'epoch': 3.0})"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"],"id":"7e15828e"},{"cell_type":"markdown","metadata":{"id":"4963a5c6"},"source":["Let's see the performance on the Common Voice test set"],"id":"4963a5c6"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":546400,"status":"ok","timestamp":1689955843092,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"},"user_tz":420},"id":"b7d88122","outputId":"25acbfeb-c9b3-46eb-a755-4463623dbab3"},"outputs":[{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'wer': 77.50512557662738, 'cer': 34.83722861823132}"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["preds = trainer.predict(test_dataset)\n","eval_preds = compute_metrics(preds)\n","eval_preds"],"id":"b7d88122"},{"cell_type":"markdown","metadata":{"id":"35718d6e"},"source":["It looks like we have a 77.5% WER and 35.8% CER. Great! We have some made some improvement after finetuning for just 3 epochs. Let's add this result to our table.\n","\n","| Model | WER % | CER %|\n","|-------|-----|----|\n","|whisper-medium|129.9|75.5|\n","|mms-1b-all|29.3|7.7|\n","|finetuned whisper-medium|77.5|35.8|\n","\n","It looks like mms-1b-all still has the best results. Let's see if further finetuning mms-1b-all will give even better results."],"id":"35718d6e"},{"cell_type":"markdown","metadata":{"id":"f6f8bb3b"},"source":["Release GPU memory for upcoming models"],"id":"f6f8bb3b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"799647fb"},"outputs":[],"source":["del model\n","torch.cuda.empty_cache()\n","\n","del trainer\n","torch.cuda.empty_cache()"],"id":"799647fb"},{"cell_type":"markdown","metadata":{"id":"a4357c4c"},"source":["### MMS"],"id":"a4357c4c"},{"cell_type":"markdown","metadata":{"id":"13210611"},"source":["We wil further fine-tune MMS to see if it can be improved by further finetuning on our Common Voice dataset. You can refer to Hugging Face's recent MMS finetuning blog for more details and explanations if needed: https://huggingface.co/blog/mms_adapters"],"id":"13210611"},{"cell_type":"markdown","metadata":{"id":"fdaf5535"},"source":["MMS-1b-all works by incorporating an adapter architecture, which are extra parameters throughout the architecture that are trainable during finetuning, and are language-specific. This enables the user to finetune a smaller number of parameters in comparison to the entire model.\n","\n","Here, we will finetune the MMS adapter weights for Hausa."],"id":"fdaf5535"},{"cell_type":"markdown","metadata":{"id":"20d2424f"},"source":["First, we will set up the tokenizer based on our previously made character vocabulary, setting special tokens for unknown characters, padding, and word delimiters according to the vocabulary. We need to specify our vocabulary for the specific language of interest in a dictionary so that the MMS-1b-all checkpoint will correctly finetune the adapter weights for Hausa."],"id":"20d2424f"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":189,"status":"ok","timestamp":1689955937943,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"},"user_tz":420},"id":"a0e1c1f8","outputId":"8bf274df-13b4-4855-de01-7f197396946e"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n","transformers.set_seed(9)\n","\n","target_lang = \"hau\"\n","\n","with open(vocab_file, \"r\") as f:\n","    vocab_dict = json.load(f)\n","\n","new_vocab_dict = {target_lang: vocab_dict}\n","\n","experiment_file = out_dir+\"mms-1b-all-finetuning-2/\"\n","\n","try:\n","    os.mkdir(experiment_file)\n","except:\n","    pass\n","\n","with open(experiment_file+\"vocab.json\", 'w') as f:\n","    json.dump(new_vocab_dict, f)\n","\n","tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(experiment_file, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\", target_lang=target_lang)"],"id":"a0e1c1f8"},{"cell_type":"markdown","metadata":{"id":"96903bd0"},"source":["Then, we will setup the feature extractor, which transforms the input audio into features. MMS takes in the raw audio, unlike the Whisper model, and simply zero-mean-unit-variance normalizes the values."],"id":"96903bd0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d755360b"},"outputs":[],"source":["feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"],"id":"d755360b"},{"cell_type":"markdown","metadata":{"id":"9f5f9b27"},"source":["And finally, the processor wraps both the tokenizer and feature extractor into one conventient class."],"id":"9f5f9b27"},{"cell_type":"code","execution_count":null,"metadata":{"id":"60fe16b0"},"outputs":[],"source":["processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"],"id":"60fe16b0"},{"cell_type":"markdown","metadata":{"id":"582b746d"},"source":["Now, we want to create a data collator (similar to the one we made for Whisper) that prepares the input in batches for the model"],"id":"582b746d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6280929"},"outputs":[],"source":["import torch\n","\n","from dataclasses import dataclass, field\n","from typing import Any, Dict, List, Optional, Union\n","\n","@dataclass\n","class DataCollatorCTCWithPadding:\n","    \"\"\"\n","    Data collator that will dynamically pad the inputs received.\n","    Args:\n","        processor (:class:`~transformers.Wav2Vec2Processor`)\n","            The processor used for proccessing the data.\n","        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n","              sequence if provided).\n","            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n","              maximum acceptable input length for the model if that argument is not provided.\n","            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n","              different lengths).\n","    \"\"\"\n","\n","    processor: Wav2Vec2Processor\n","    padding: Union[bool, str] = True\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # split inputs and labels since they have to be of different lenghts and need\n","        # different padding methods\n","        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","\n","        batch = self.processor.pad(\n","            input_features,\n","            padding=self.padding,\n","            return_tensors=\"pt\",\n","        )\n","        labels_batch = self.processor.pad(\n","            labels=label_features,\n","            padding=self.padding,\n","            return_tensors=\"pt\",\n","        )\n","\n","        # replace padding with -100 to ignore loss correctly\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch\n","\n","data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"],"id":"a6280929"},{"cell_type":"markdown","metadata":{"id":"df23e31b"},"source":["Now we create an evaluation function."],"id":"df23e31b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbc17ab5"},"outputs":[],"source":["import numpy as np\n","def compute_metrics(pred):\n","    wer_metric = evaluate.load(\"wer\")\n","    cer_metric = evaluate.load(\"cer\")\n","    pred_logits = pred.predictions\n","    pred_ids = np.argmax(pred_logits, axis=-1)\n","\n","    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n","\n","    pred_str = processor.batch_decode(pred_ids)\n","    # we do not want to group tokens when computing the metrics\n","    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n","\n","    wer = 100*wer_metric.compute(predictions=pred_str, references=label_str)\n","    cer = 100*cer_metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"wer\": wer, \"cer\": cer}"],"id":"bbc17ab5"},{"cell_type":"markdown","metadata":{"id":"a22618f1"},"source":["Now, we can define the model."],"id":"a22618f1"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20114,"status":"ok","timestamp":1689955965682,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"},"user_tz":420},"id":"69136f31","outputId":"05ca5f45-c7b8-4769-ff3c-a3d0af2cb08d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized because the shapes did not match:\n","- lm_head.bias: found shape torch.Size([154]) in the checkpoint and torch.Size([47]) in the model instantiated\n","- lm_head.weight: found shape torch.Size([154, 1280]) in the checkpoint and torch.Size([47, 1280]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import Wav2Vec2ForCTC\n","\n","model_card = \"facebook/mms-1b-all\"\n","model = Wav2Vec2ForCTC.from_pretrained(\n","    \"facebook/mms-1b-all\",\n","    attention_dropout=0.0,\n","    hidden_dropout=0.0,\n","    feat_proj_dropout=0.0,\n","    layerdrop=0.0,\n","    ctc_loss_reduction=\"mean\",\n","    pad_token_id=processor.tokenizer.pad_token_id,\n","    vocab_size=len(processor.tokenizer),\n","    ignore_mismatched_sizes=True,\n",")"],"id":"69136f31"},{"cell_type":"markdown","metadata":{"id":"a36ce7ec"},"source":["We re-initialize the adapter layers to prepare for finetuning"],"id":"a36ce7ec"},{"cell_type":"code","execution_count":null,"metadata":{"id":"87198e0e"},"outputs":[],"source":["model.init_adapter_layers()"],"id":"87198e0e"},{"cell_type":"markdown","metadata":{"id":"54a695c7"},"source":["Then we freeze all the parameters (learned from the pretraining and finetuning by the Meta team) except the adapter weights"],"id":"54a695c7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4e9fa3c9"},"outputs":[],"source":["model.freeze_base_model()\n","\n","adapter_weights = model._get_adapters()\n","for param in adapter_weights.values():\n","    param.requires_grad = True"],"id":"4e9fa3c9"},{"cell_type":"markdown","metadata":{"id":"6256f550"},"source":["Then, we set up the parameters for model training like for Whisper"],"id":"6256f550"},{"cell_type":"code","execution_count":null,"metadata":{"id":"18563719"},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\n","  output_dir=experiment_file,\n","  group_by_length=True,\n","  per_device_train_batch_size=8,\n","  evaluation_strategy=\"steps\",\n","  num_train_epochs=3,\n","  gradient_checkpointing=True, # another way to save GPU memory by recomputing gradients (less memory, more time)\n","  fp16=True, # this enables mixed precision training, which lets some data be stored in 16 bit floating point precision instead of 32 bits.\n","  save_steps=200,\n","  eval_steps=100,\n","  logging_steps=100,\n","  learning_rate=1e-3,\n","  warmup_steps=100,\n","  save_total_limit=2,\n","  push_to_hub=False,\n","  load_best_model_at_end=True,\n","  metric_for_best_model=\"wer\",\n","  greater_is_better=False\n",")"],"id":"18563719"},{"cell_type":"markdown","metadata":{"id":"cf38c93a"},"source":["Then send everything to the Trainer class for training!"],"id":"cf38c93a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"702443fd"},"outputs":[],"source":["# since our processor is different, we will need to create new ASRDataset objects\n","train_dataset = ASRDatasetWav2Vec2(train_audio, train_transcripts, 16000, processor)\n","val_dataset = ASRDatasetWav2Vec2(val_audio,  val_transcripts, 16000, processor)\n","test_dataset = ASRDatasetWav2Vec2(test_audio, test_transcripts, 16000, processor)"],"id":"702443fd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"86677411"},"outputs":[],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=processor.feature_extractor,\n",")"],"id":"86677411"},{"cell_type":"markdown","metadata":{"id":"bb7f9691"},"source":["Training 3 epochs with batch size 8 takes about 30 minutes."],"id":"bb7f9691"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":545},"executionInfo":{"elapsed":1728439,"status":"ok","timestamp":1689957696974,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"},"user_tz":420},"id":"0c2e53e9","outputId":"e6fc4080-eedf-40ba-f432-2c47b35c7f53","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1224' max='1224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1224/1224 28:34, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Wer</th>\n","      <th>Cer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>7.233400</td>\n","      <td>3.345960</td>\n","      <td>99.387783</td>\n","      <td>89.554847</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>3.047000</td>\n","      <td>2.922044</td>\n","      <td>98.497287</td>\n","      <td>89.476350</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>2.830900</td>\n","      <td>2.850453</td>\n","      <td>96.006679</td>\n","      <td>86.774537</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>2.740300</td>\n","      <td>2.731788</td>\n","      <td>93.154306</td>\n","      <td>74.339107</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>2.650300</td>\n","      <td>2.612951</td>\n","      <td>99.874774</td>\n","      <td>65.527195</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>2.592300</td>\n","      <td>2.568540</td>\n","      <td>95.157924</td>\n","      <td>66.694520</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>2.532200</td>\n","      <td>2.520914</td>\n","      <td>97.648532</td>\n","      <td>65.180290</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>2.504900</td>\n","      <td>2.546631</td>\n","      <td>98.678169</td>\n","      <td>62.838043</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>2.480400</td>\n","      <td>2.485876</td>\n","      <td>98.566857</td>\n","      <td>64.253520</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>2.459500</td>\n","      <td>2.483927</td>\n","      <td>99.624322</td>\n","      <td>63.091259</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>2.376900</td>\n","      <td>2.370473</td>\n","      <td>97.634618</td>\n","      <td>59.462676</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>1.991400</td>\n","      <td>1.698764</td>\n","      <td>91.874217</td>\n","      <td>38.774942</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=1224, training_loss=2.9247812383315144, metrics={'train_runtime': 1723.7315, 'train_samples_per_second': 5.672, 'train_steps_per_second': 0.71, 'total_flos': 1.4065565190591801e+19, 'train_loss': 2.9247812383315144, 'epoch': 3.0})"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"],"id":"0c2e53e9"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":75029,"status":"ok","timestamp":1689958033931,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"},"user_tz":420},"id":"206f189d","outputId":"85d9b4dc-fe44-4a60-9b7f-0956aa41a2a1"},"outputs":[{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'wer': 92.11841599384853, 'cer': 39.67283466383647}"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["preds = trainer.predict(test_dataset)\n","eval_preds = compute_metrics(preds)\n","eval_preds"],"id":"206f189d"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"executionInfo":{"elapsed":5,"status":"error","timestamp":1689961206543,"user":{"displayName":"Kaleen Shrestha","userId":"17677293658466379584"},"user_tz":420},"id":"MYyiZnB9iXKW","outputId":"0e2155c7-9256-40db-e143-bf4659d00983"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-91ab8b67f715>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'eval_preds' is not defined"]}],"source":["eval_preds"],"id":"MYyiZnB9iXKW"},{"cell_type":"markdown","metadata":{"id":"fe4244c8"},"source":["It looks like we have a 28.3% WER and 7.9% CER. Perhaps a different set of hyperparameters (such as learning rate, batch size, epochs) would show better results. Or the data does not have more information that MMS can learn. Please refer to Section C for guidance on how to experiment with different hyperparameters. Let's add this result to our table.\n","\n","| Model | WER % | CER %|\n","|-------|-----|----|\n","|whisper-medium|129.9|75.5|\n","|mms-1b-all|29.3|7.7|\n","|finetuned whisper-medium|77.5|35.8|\n","|finetuned mms-1b-all|28.3|7.9|"],"id":"fe4244c8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e7c5715a"},"outputs":[],"source":["del model\n","torch.cuda.empty_cache()\n","\n","del trainer\n","torch.cuda.empty_cache()"],"id":"e7c5715a"},{"cell_type":"markdown","metadata":{"id":"36e91f3a"},"source":["### XLS-R"],"id":"36e91f3a"},{"cell_type":"markdown","metadata":{"id":"ba222f69"},"source":["XLS-R was released before MMS, and the MMS paper claims (CHECK) that it has better performance than XLS-R. However, it may be a good idea to check to see which model is better for your specific dataset and use-case. Therefore, let's try finetuning XLS-R on the Hausa fleurs dataset. Refer to the [Hugging Face tutorial](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2) for more details."],"id":"ba222f69"},{"cell_type":"markdown","metadata":{"id":"444b4d1d"},"source":["Similar to MMS, we will create a tokenizer from the character vocabulary file we made earlier in this tutorial, then the feature extractor and processor that wraps the tokenizer and feature extractor."],"id":"444b4d1d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5d812a99"},"outputs":[],"source":["from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2ForCTC, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n","transformers.set_seed(9)\n","\n","model_sampling_rate = 16000\n","tokenizer = Wav2Vec2CTCTokenizer(vocab_file, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n","\n","feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=model_sampling_rate, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n","\n","processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"],"id":"5d812a99"},{"cell_type":"markdown","metadata":{"id":"2f49d290"},"source":["Then, we want to create our dataset objects using our processor."],"id":"2f49d290"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3a404458"},"outputs":[],"source":["train_dataset = ASRDatasetWav2Vec2(train_audio, train_transcripts, model_sampling_rate, processor)\n","val_dataset = ASRDatasetWav2Vec2(val_audio,  val_transcripts, model_sampling_rate, processor)\n","test_dataset = ASRDatasetWav2Vec2(test_audio, test_transcripts, model_sampling_rate, processor)"],"id":"3a404458"},{"cell_type":"markdown","metadata":{"id":"e53f99c5"},"source":["Then, we want to instantiate a data collator of the same class as the one for MMS"],"id":"e53f99c5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a95eccff"},"outputs":[],"source":["data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"],"id":"a95eccff"},{"cell_type":"markdown","metadata":{"id":"c34c355b"},"source":["Now, we create variables and functions for training"],"id":"c34c355b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bc34870"},"outputs":[],"source":["import numpy as np\n","def compute_metrics(pred):\n","    wer_metric = evaluate.load(\"wer\")\n","    cer_metric = evaluate.load(\"cer\")\n","\n","    pred_logits = pred.predictions\n","    pred_ids = np.argmax(pred_logits, axis=-1)\n","\n","    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n","\n","    pred_str = processor.batch_decode(pred_ids)\n","    # we do not want to group tokens when computing the metrics\n","    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n","\n","    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n","    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"wer\": wer, \"cer\": cer}"],"id":"6bc34870"},{"cell_type":"markdown","metadata":{"id":"4c3859b2"},"source":["For example reasons, we will use the XLS-R checkpoint with 300 million parameters. For better comparison with MMS-1b, it would be better to use [XLS-R with 1 billion parameters](https://huggingface.co/facebook/wav2vec2-xls-r-1b) which requires more GPU memory."],"id":"4c3859b2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2118f81a","outputId":"6da0f14a-2ccc-44cc-bc6b-b4bd315036ce"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at facebook/wav2vec2-xls-r-300m were not used when initializing Wav2Vec2ForCTC: ['quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_q.weight', 'quantizer.weight_proj.weight', 'project_q.bias', 'project_hid.bias', 'project_hid.weight']\n","- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1890: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n","  warnings.warn(\n"]}],"source":["model = Wav2Vec2ForCTC.from_pretrained(\n","    \"facebook/wav2vec2-xls-r-300m\",\n","    attention_dropout=0.1,\n","    hidden_dropout=0.1,\n","    feat_proj_dropout=0.0,\n","    mask_time_prob=0.05,\n","    layerdrop=0.1,\n","    ctc_loss_reduction=\"mean\",\n","    pad_token_id=processor.tokenizer.pad_token_id,\n","    vocab_size=len(processor.tokenizer),\n","    ignore_mismatched_sizes=True\n",")\n","\n","model.freeze_feature_extractor()\n","model.gradient_checkpointing_enable()"],"id":"2118f81a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"998d22bf"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","training_args = TrainingArguments(\n","  output_dir=out_dir+\"xls-r-300m-experiment-1\",\n","  group_by_length=True,\n","  per_device_train_batch_size=8,\n","  gradient_accumulation_steps=4,\n","  evaluation_strategy=\"steps\",\n","  num_train_epochs=3,\n","  fp16=True,\n","  save_steps=100,\n","  eval_steps=100,\n","  logging_steps=20,\n","  learning_rate=2e-4,\n","  warmup_steps=500,\n","  save_total_limit=2,\n","  metric_for_best_model=\"wer\",\n","  greater_is_better=False,\n","  load_best_model_at_end=True\n",")"],"id":"998d22bf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ea426a7"},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=processor.feature_extractor,\n",")"],"id":"4ea426a7"},{"cell_type":"markdown","metadata":{"id":"d3b063e0"},"source":["Training with batch size of 32 and 10 epochs takes about 9 minutes and 30 GB on an NVIDIA A100 GPU."],"id":"d3b063e0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"14dbb7e8","outputId":"0738b41b-ddcf-4b13-c4b3-7691745e485f"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [306/306 09:14, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Wer</th>\n","      <th>Cer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>5.874400</td>\n","      <td>5.435220</td>\n","      <td>1.000000</td>\n","      <td>0.996632</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>2.992300</td>\n","      <td>3.060462</td>\n","      <td>1.000000</td>\n","      <td>0.996632</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>2.847700</td>\n","      <td>2.922175</td>\n","      <td>1.000000</td>\n","      <td>0.996632</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=306, training_loss=6.491241218217837, metrics={'train_runtime': 562.1895, 'train_samples_per_second': 17.391, 'train_steps_per_second': 0.544, 'total_flos': 4.53984711058008e+18, 'train_loss': 6.491241218217837, 'epoch': 3.0})"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"],"id":"14dbb7e8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ff447a7a","outputId":"2000cc58-7c98-4fc0-d3be-0e3b2174c4b3"},"outputs":[{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'wer': 1.0, 'cer': 0.9968390937197176}"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["preds = trainer.predict(test_dataset)\n","eval_preds = compute_metrics(preds)\n","eval_preds"],"id":"ff447a7a"},{"cell_type":"markdown","metadata":{"id":"19ceb5d1"},"source":["It looks like we have a 100% WER and 99.7% CER. Let's add this result to our table.\n","\n","| Model | WER % | CER %|\n","|-------|-----|----|\n","|whisper-large-v2| 97.8| 40.5|\n","|mms-1b-all|29.3|7.7|\n","|finetuned whisper-large-v2|40.4|19.2|\n","|finetuned mms-1b-all|27.9|8.1|\n","|finetuned xls-r-300m|100|99.7|\n","\n","**Note** We found that the loss for XLS-R on Hausa stabalizes after around 10 epochs, so train for 10+ epochs for better results. 3 epochs is for example purposes."],"id":"19ceb5d1"},{"cell_type":"markdown","metadata":{"id":"75943484"},"source":["# Under Construction"],"id":"75943484"},{"cell_type":"markdown","metadata":{"id":"f08ada4a"},"source":["## Section C: Further Improvements"],"id":"f08ada4a"},{"cell_type":"markdown","metadata":{"id":"f9f18ecc"},"source":["### Available scripts"],"id":"f9f18ecc"},{"cell_type":"markdown","metadata":{"id":"6afc8b74"},"source":["For convenience, we have provided in this GitHub repo a finetuning script `finetuning.py` that enables the user to enter any FLUERS language or custom prepared dataset, and model training hyperparameters to do finetuning and evaluation all in one easy script.\n","\n","**Note** Make sure you are using a machine with access to a terminal so you can run the Python script.\n","\n","Using FLEURS only\n","```\n","python finetuning.py --fluers_language_code ha_ng --preprocessing_function preprocess --model \"facebook/mms-1b-all\"\n","\n","```\n","\n","Using a custom dataset\n","```\n","python finetuning.py ----custom_dataset_function custom_dataset --preprocessing_function preprocess --model \"facebook/mms-1b-all\"\n","\n","```\n","\n","For the preprocessing function argument, you will need to create a Python script (e.g. `preprocess.py`) that has a `preprocess()` function that takes in a List of strings and outputs a processed List of strings:\n","\n","**preprocess.py**\n","```\n","from typing import List\n","\n","def preprocess(List[str] transcriptions) -> List[str]:\n","    cleaned_transformations = your_transformation(transcriptions)\n","    return cleaned_transformations\n","```\n","\n","See the \"Adding More Data\" section for instructions on how to define a custom dataset script."],"id":"6afc8b74"},{"cell_type":"markdown","metadata":{"id":"5ee3831d"},"source":["#### Adding More Data"],"id":"5ee3831d"},{"cell_type":"markdown","metadata":{"id":"34b95d37"},"source":["In order to use a dataset other than FLEURS, you must make sure to set up a Python script that has a function called `create_dataset()`. It must return three ASRDataset objects for the training, validation, and test set. The ASRDataset is available in the `utilities.py` script.\n","\n","*Example custom dataset script:*\n","\n","```\n","from utilities import ASRDataset\n","def create_dataset() -> Tuple[ASRDataset]:\n","    # your code\n","    train_dataset = ASRDataset(audio_train, transcripts_train, sampling_rate, processor)\n","    val_dataset = ASRDataset(audio_val, transcripts_val, sampling_rate, processor)\n","    test_dataset = ASRDataset(audio_test, transcripts_test, sampling_rate, processor)\n","\n","    return train_dataset, val_dataset, test_dataset\n","```\n","\n","This option applies for when you want to combine FLEURS with another dataset as well."],"id":"34b95d37"},{"cell_type":"markdown","metadata":{"id":"a153feed"},"source":["#### Hyperparameter tuning"],"id":"a153feed"},{"cell_type":"markdown","metadata":{"id":"bab900e4"},"source":["Using the scripts available in this GitHub repo, you can run your own experiments with different hyperparameters to see what gives the best model performance."],"id":"bab900e4"}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b357618388cf4e09be5cbfbfe3b6d4da":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d17999c9b3d744f3a038879aafdbea13","IPY_MODEL_71a342a18ddc4051b22b70284fbb5384","IPY_MODEL_6980111d4d704f2c8df3bff2fe3207f6"],"layout":"IPY_MODEL_d55196e986084aef9ba0109ef3d878a5"}},"d17999c9b3d744f3a038879aafdbea13":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8947606c562d498fbca60413c2d8f9e9","placeholder":"​","style":"IPY_MODEL_940b1180514343b1990727a434f2b345","value":"Downloading builder script: 100%"}},"71a342a18ddc4051b22b70284fbb5384":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b5ec84a8a9948f5aae868af7d2503eb","max":4485,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8007b98d91344fcaa6ec56514e85b96e","value":4485}},"6980111d4d704f2c8df3bff2fe3207f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_248e1e917db648c7822f6da61329deae","placeholder":"​","style":"IPY_MODEL_f760b0e03c74488a83c3a217ebf33bc7","value":" 4.49k/4.49k [00:00&lt;00:00, 353kB/s]"}},"d55196e986084aef9ba0109ef3d878a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8947606c562d498fbca60413c2d8f9e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"940b1180514343b1990727a434f2b345":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b5ec84a8a9948f5aae868af7d2503eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8007b98d91344fcaa6ec56514e85b96e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"248e1e917db648c7822f6da61329deae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f760b0e03c74488a83c3a217ebf33bc7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7197a0f55cd04775974d89d1c0cb17ec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc21e88298f648eaad07a42ff1de4f28","IPY_MODEL_c3de9ff702304d7bad85d35159a2b13e","IPY_MODEL_3457432e869044e4a51ab9c278b7a9ad"],"layout":"IPY_MODEL_c20d520690e94688b9e40ddc56faa868"}},"bc21e88298f648eaad07a42ff1de4f28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d641c697e39d4d95b9106f907eeef87b","placeholder":"​","style":"IPY_MODEL_3f2ba54ed1244fd6bc6905e822e2ea2a","value":"Downloading builder script: 100%"}},"c3de9ff702304d7bad85d35159a2b13e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6dd17e1ccda04467a238ea4891824fd1","max":5599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_33be0b8885594343be102192f2fb38de","value":5599}},"3457432e869044e4a51ab9c278b7a9ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5aefdd3d6bc8408a81abeebc70b0192f","placeholder":"​","style":"IPY_MODEL_8ab5d2901f5b4e369f56c8e08e2c8e50","value":" 5.60k/5.60k [00:00&lt;00:00, 491kB/s]"}},"c20d520690e94688b9e40ddc56faa868":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d641c697e39d4d95b9106f907eeef87b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f2ba54ed1244fd6bc6905e822e2ea2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6dd17e1ccda04467a238ea4891824fd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33be0b8885594343be102192f2fb38de":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5aefdd3d6bc8408a81abeebc70b0192f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ab5d2901f5b4e369f56c8e08e2c8e50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64c6ce7f160c47b491bc6a20fec1fe3f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_665677b287dc43518acf87289411e908","IPY_MODEL_ab10e506999e45fe8a02c0a9d5b555c4","IPY_MODEL_8bd7f57208394a50a392e6efe5b1a17d"],"layout":"IPY_MODEL_b45eb56065ea42e880489738d504fd8b"}},"665677b287dc43518acf87289411e908":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dd31774a97c403e905bf0ee2f5580ac","placeholder":"​","style":"IPY_MODEL_5032523644de473e94d8a97aa7c8b7c9","value":"Downloading (…)rocessor_config.json: 100%"}},"ab10e506999e45fe8a02c0a9d5b555c4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_93fca2dc97974703b6539e6af75bf602","max":254,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0eaa0bfe627499ea1c22f255d5adea4","value":254}},"8bd7f57208394a50a392e6efe5b1a17d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a25418810674f7093c888d6446d1600","placeholder":"​","style":"IPY_MODEL_6cdf364020204ad4848353c79f145455","value":" 254/254 [00:00&lt;00:00, 19.9kB/s]"}},"b45eb56065ea42e880489738d504fd8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dd31774a97c403e905bf0ee2f5580ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5032523644de473e94d8a97aa7c8b7c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93fca2dc97974703b6539e6af75bf602":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0eaa0bfe627499ea1c22f255d5adea4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a25418810674f7093c888d6446d1600":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cdf364020204ad4848353c79f145455":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f9812efe93a410a8b6ff0b703ed9107":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_448269ead4104085a1c0a8e59e32a9a3","IPY_MODEL_0828860a87b7474a882ca3f91ccb1dd9","IPY_MODEL_348a9c3d036446c6b936cb73f61e7c6c"],"layout":"IPY_MODEL_94974fbfaaa8460a8ef39bda41ddf61c"}},"448269ead4104085a1c0a8e59e32a9a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a42fe367dfe44e393d7a70eee720e06","placeholder":"​","style":"IPY_MODEL_30bbf28fcbcb4e5685cfc7ec407ca66f","value":"Downloading (…)okenizer_config.json: 100%"}},"0828860a87b7474a882ca3f91ccb1dd9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_32b093ba8c6f4af498b2657123467248","max":397,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f8a31cfbab934a6ca363841830d4c80c","value":397}},"348a9c3d036446c6b936cb73f61e7c6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c583d252107c4ea587f7af8afdc9c5d9","placeholder":"​","style":"IPY_MODEL_8f21ba95fd944c2581d382778ab316ab","value":" 397/397 [00:00&lt;00:00, 34.2kB/s]"}},"94974fbfaaa8460a8ef39bda41ddf61c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a42fe367dfe44e393d7a70eee720e06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30bbf28fcbcb4e5685cfc7ec407ca66f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32b093ba8c6f4af498b2657123467248":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8a31cfbab934a6ca363841830d4c80c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c583d252107c4ea587f7af8afdc9c5d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f21ba95fd944c2581d382778ab316ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f620f13c1c844c6f86a489dc9fa139a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_862ef0d78f074a678a76140d892292e2","IPY_MODEL_45a9410445204c27a9981aba02e510ab","IPY_MODEL_dda44a36994e4ef69fb50b146cbe3c61"],"layout":"IPY_MODEL_80f290e382434287b92c15d9c6720213"}},"862ef0d78f074a678a76140d892292e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30b9afafb04e4398b0e1358c8c8b9746","placeholder":"​","style":"IPY_MODEL_1f735c9c15ac45ee978890521dc4f44e","value":"Downloading (…)olve/main/vocab.json: 100%"}},"45a9410445204c27a9981aba02e510ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee6c870a2b3243b8b37cbe1576a27448","max":1340148,"min":0,"orientation":"horizontal","style":"IPY_MODEL_824ba2b5e8204973b03e4110fe1d7141","value":1340148}},"dda44a36994e4ef69fb50b146cbe3c61":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f2bd9667b2f40f79562753e78074a0f","placeholder":"​","style":"IPY_MODEL_8ebc59bc7b3e4130ab76a270ec1ebb47","value":" 1.34M/1.34M [00:01&lt;00:00, 1.23MB/s]"}},"80f290e382434287b92c15d9c6720213":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30b9afafb04e4398b0e1358c8c8b9746":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f735c9c15ac45ee978890521dc4f44e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee6c870a2b3243b8b37cbe1576a27448":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"824ba2b5e8204973b03e4110fe1d7141":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5f2bd9667b2f40f79562753e78074a0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ebc59bc7b3e4130ab76a270ec1ebb47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"288b1480250148289318e1ba88ad6093":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d7638dd1877244318525421c622c7f10","IPY_MODEL_2092da2ef1da4dcabd76628d2679b066","IPY_MODEL_8652f78d68e3435dab1b9055add8b483"],"layout":"IPY_MODEL_c6a7a258c8d546b3a54033f7e5e18e89"}},"d7638dd1877244318525421c622c7f10":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dde7a6a3dd04f14968b3cfabec4724a","placeholder":"​","style":"IPY_MODEL_88cf7f3e4beb40dc92c04d83ee583aca","value":"Downloading (…)cial_tokens_map.json: 100%"}},"2092da2ef1da4dcabd76628d2679b066":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_952a819f1ed440dbbdc2c92e45cd3036","max":96,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7565f20585764151a73295223ea3b235","value":96}},"8652f78d68e3435dab1b9055add8b483":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3b60735d5804771be195dfef0ef573f","placeholder":"​","style":"IPY_MODEL_83de379564f541d49dd50741152ba8a0","value":" 96.0/96.0 [00:00&lt;00:00, 9.09kB/s]"}},"c6a7a258c8d546b3a54033f7e5e18e89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dde7a6a3dd04f14968b3cfabec4724a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88cf7f3e4beb40dc92c04d83ee583aca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"952a819f1ed440dbbdc2c92e45cd3036":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7565f20585764151a73295223ea3b235":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3b60735d5804771be195dfef0ef573f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83de379564f541d49dd50741152ba8a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e6529c49c694bc18292df229f78a0d4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_99f9fadfd82840ff9141335512b5aa1e","IPY_MODEL_b392c970a92c4c9a823ea333fa462a75","IPY_MODEL_f3057681a31249caa49b2d6ea8bdd7cb"],"layout":"IPY_MODEL_b88667fbb8c643a381ecf5f3fbb24643"}},"99f9fadfd82840ff9141335512b5aa1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3111ee71fd4b4aa28930bbbd8206333a","placeholder":"​","style":"IPY_MODEL_0c549e2fa8354f4da4a550af97e8440c","value":"Downloading (…)lve/main/config.json: 100%"}},"b392c970a92c4c9a823ea333fa462a75":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7eda0d07b47401f92b3a283fe6eb0c4","max":2040,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b2694e8de5c4d7282c7f39c76df52ef","value":2040}},"f3057681a31249caa49b2d6ea8bdd7cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75a458feb0af407eb8edaac3cef0f858","placeholder":"​","style":"IPY_MODEL_e4ac24638430410fb03b1cb09993780c","value":" 2.04k/2.04k [00:00&lt;00:00, 177kB/s]"}},"b88667fbb8c643a381ecf5f3fbb24643":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3111ee71fd4b4aa28930bbbd8206333a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c549e2fa8354f4da4a550af97e8440c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7eda0d07b47401f92b3a283fe6eb0c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b2694e8de5c4d7282c7f39c76df52ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"75a458feb0af407eb8edaac3cef0f858":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4ac24638430410fb03b1cb09993780c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8710b75ce5d64c94822334668ef82e84":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b136d9fd4e640ea9f2a42c786537e32","IPY_MODEL_8bdee04c4cee42ca80ad28589908e817","IPY_MODEL_db6f7a46f0134f71a0c81e248f28a031"],"layout":"IPY_MODEL_6c304820e972458ea0529b0348fe9f2f"}},"4b136d9fd4e640ea9f2a42c786537e32":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_834341f488d945738c3d9fd505bcff46","placeholder":"​","style":"IPY_MODEL_7950db7975c34d30b0288e7191e18307","value":"Downloading model.safetensors: 100%"}},"8bdee04c4cee42ca80ad28589908e817":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09472eb97a7741f8a68253e23c940277","max":3859521128,"min":0,"orientation":"horizontal","style":"IPY_MODEL_560e235f3e5545dc861745c6e9c69160","value":3859521128}},"db6f7a46f0134f71a0c81e248f28a031":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eec91ceb8f434d54bc92d73dfe2759b8","placeholder":"​","style":"IPY_MODEL_a1fadda70dd34f78be74f16ed64711e5","value":" 3.86G/3.86G [00:12&lt;00:00, 352MB/s]"}},"6c304820e972458ea0529b0348fe9f2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"834341f488d945738c3d9fd505bcff46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7950db7975c34d30b0288e7191e18307":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09472eb97a7741f8a68253e23c940277":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"560e235f3e5545dc861745c6e9c69160":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eec91ceb8f434d54bc92d73dfe2759b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1fadda70dd34f78be74f16ed64711e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ce9ba374992451da81860ff974e741f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f11061d98fa4aa0b7098c82500beaac","IPY_MODEL_50f4a21fe20d4ace820b5b991e8debce","IPY_MODEL_0b8375de02e84e47a488461f6d780086"],"layout":"IPY_MODEL_ee2d974deb6e4c59b4d5def3ac89e6df"}},"8f11061d98fa4aa0b7098c82500beaac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecd1e6f057964e62b4d696fbdfa5e1fa","placeholder":"​","style":"IPY_MODEL_58a9e63ee68d4a54b8a9938f90fe4f27","value":"Downloading (…)pter.hau.safetensors: 100%"}},"50f4a21fe20d4ace820b5b991e8debce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e2bf246e6f4422289b9526b1ddf8e3c","max":9070120,"min":0,"orientation":"horizontal","style":"IPY_MODEL_44f32366ace3408ead280bec9c2a2b37","value":9070120}},"0b8375de02e84e47a488461f6d780086":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecd824e7f0324eff91e4deea43c9d17c","placeholder":"​","style":"IPY_MODEL_992badfaab714f39897fbd5d2c93f049","value":" 9.07M/9.07M [00:04&lt;00:00, 1.98MB/s]"}},"ee2d974deb6e4c59b4d5def3ac89e6df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecd1e6f057964e62b4d696fbdfa5e1fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58a9e63ee68d4a54b8a9938f90fe4f27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e2bf246e6f4422289b9526b1ddf8e3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44f32366ace3408ead280bec9c2a2b37":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ecd824e7f0324eff91e4deea43c9d17c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"992badfaab714f39897fbd5d2c93f049":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}
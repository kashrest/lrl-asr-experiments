{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ea45ff",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition (ASR) Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b82bb",
   "metadata": {},
   "source": [
    "## Fine-tune a pretrained, multilingual ASR model on FLEURS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0545f5",
   "metadata": {},
   "source": [
    "In this tutorial, we will be evaluating and improving a multilingual ASR model for a language in the FLEURS dataset. We will focus on **Hausa**, but you can follow along in any language in FLEURS. See the FLEURS dataset paper for a list of supported languages: https://arxiv.org/abs/2205.12446.\n",
    "\n",
    "We will be looking at three major open-source ASR multilingual models:\n",
    "* XLS-R  (https://arxiv.org/abs/2111.09296)\n",
    "* Whisper (https://cdn.openai.com/papers/whisper.pdf)\n",
    "* MMS (https://scontent-sjc3-1.xx.fbcdn.net/v/t39.8562-6/348827959_6967534189927933_6819186233244071998_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=-JOSFMsFL-UAX-4O6o4&_nc_ht=scontent-sjc3-1.xx&oh=00_AfDdMFq0DP2xIRyjWpGrmIpqncnouiylLfWnFsAgxboLWw&oe=6497E242)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945531e",
   "metadata": {},
   "source": [
    "## Before you start: Setting up your coding environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7e5c2",
   "metadata": {},
   "source": [
    "You can run follow along and run the lines of code in this notebook, and also utilize the scripts found in this GitHub respository. Before starting this tutorial, you will need to create a virtual environment for this project so you can download all the required packages without affecting your other projects. We recommend using Anaconda (conda) to create a virtual environment. We have provided an `environment.yml` file that you can use to create a virtual environment named `asr` containing all the required packages. In your terminal run this code:\n",
    "\n",
    "```\n",
    "git clone https://github.com/kashrest/lrl-asr-experiments.git\n",
    "cd lrl-asr-experiments\n",
    "conda env create -f environment.yml \n",
    "conda activate asr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fa7f76",
   "metadata": {},
   "source": [
    "Note: The pretrained multilingual ASR models we will be using in this notebook require GPUs with at least 40 GB of space (check) for practical use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e13d8d",
   "metadata": {},
   "source": [
    "Now you can run the lines in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71af9f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbbef6",
   "metadata": {},
   "source": [
    "The first step is to download and prepare the data for the ASR model. Hugging Face has an easy way to download FLEURS data for any supported language, where the split can be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24889296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fleurs (/data/users/kashrest/lrl-asr-experiments/data/fleurs/google___fleurs/ha_ng/2.0.0/af82dbec419a815084fa63ebd5d5a9f24a6e9acdf9887b9e3b8c6bbd64e0b7ac)\n",
      "Found cached dataset fleurs (/data/users/kashrest/lrl-asr-experiments/data/fleurs/google___fleurs/ha_ng/2.0.0/af82dbec419a815084fa63ebd5d5a9f24a6e9acdf9887b9e3b8c6bbd64e0b7ac)\n",
      "Found cached dataset fleurs (/data/users/kashrest/lrl-asr-experiments/data/fleurs/google___fleurs/ha_ng/2.0.0/af82dbec419a815084fa63ebd5d5a9f24a6e9acdf9887b9e3b8c6bbd64e0b7ac)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir_fleurs = \"./data/fleurs/\"\n",
    "out_dir = \"./asr-tutorial-experiment-1/\"\n",
    "\n",
    "# create a data directory for caching \n",
    "try:\n",
    "    os.mkdir(cache_dir_fleurs)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# create a directory for outputs \n",
    "try:\n",
    "    os.mkdir(out_dir)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# for Hausa, the language code is \"ha_ng\"\n",
    "train_data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"train\", cache_dir=cache_dir_fleurs)\n",
    "val_data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"validation\", cache_dir=cache_dir_fleurs)\n",
    "test_data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"test\", cache_dir=cache_dir_fleurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8752ed3a",
   "metadata": {},
   "source": [
    "FLEURS data is organized like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab66092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 302,\n",
       " 'num_samples': 301440,\n",
       " 'path': '/data/users/kashrest/asr-experiments/downloads/extracted/953575415c600d2042020b042380119265aefaa4fcf95afc300adfcd2b79784e/10002175198254707815.wav',\n",
       " 'audio': {'path': 'train/10002175198254707815.wav',\n",
       "  'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         6.78300858e-05, 1.26361847e-05, 6.46114349e-05]),\n",
       "  'sampling_rate': 16000},\n",
       " 'transcription': 'nasarorin da vautier ta samu marasa alaka da bada umarni ba sun hada da yajin cin abinci a 1973 a kan abin da ya ke ganin dabaibayin siyasa ne',\n",
       " 'raw_transcription': 'Nasarorin da Vautier ta samu marasa alaka da bada umarni ba sun hada da yajin cin abinci a 1973 a kan abin da ya ke ganin dabaibayin siyasa ne.',\n",
       " 'gender': 1,\n",
       " 'lang_id': 30,\n",
       " 'language': 'Hausa',\n",
       " 'lang_group_id': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec16d08d",
   "metadata": {},
   "source": [
    "We are interested in the audio (represented as an array of floats each describing the amplitude/loudness of the sound; the number of floats is determined by the sampling rate which here is 16,000 measurements per second) and the corresponding transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d6e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transcripts, val_transcripts, test_transcripts = [], [], []\n",
    "train_audio, val_audio, test_audio = [], [], []\n",
    "\n",
    "for elem in train_data:\n",
    "    train_audio.append(elem[\"audio\"][\"array\"])\n",
    "    train_transcripts.append(elem[\"raw_transcription\"])\n",
    "    \n",
    "for elem in val_data:\n",
    "    val_audio.append(elem[\"audio\"][\"array\"])\n",
    "    val_transcripts.append(elem[\"raw_transcription\"])\n",
    "    \n",
    "for elem in test_data:\n",
    "    test_audio.append(elem[\"audio\"][\"array\"])\n",
    "    test_transcripts.append(elem[\"raw_transcription\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945c231",
   "metadata": {},
   "source": [
    "Now, since we are interested in transcribing speech, we want to clean the transcripts by removing special characters that do not have a clear sound (such as ! '). This part may depend on your target application and language. For example for Hausa, many native speakers do not speak English and does not have much code-switching, so we want to also normalize any foreign characters (Ã§ ÅŸ) and symbols (% & $)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2158c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_texts_hausa(transcriptions):\n",
    "    chars_to_remove_regex = '[\\,\\?\\!\\-\\;\\:\\\"\\â€œ\\%\\â€˜\\'\\Ê»\\â€\\ï¿½\\$\\&\\(\\)\\â€“\\â€”]'\n",
    "\n",
    "    def _remove_special_characters(transcription):\n",
    "        transcription = transcription.strip()\n",
    "        transcription = transcription.lower()\n",
    "        transcription = re.sub(chars_to_remove_regex, '', transcription)\n",
    "        transcription = re.sub(\"\\[\\]\\{\\}\", '', transcription)\n",
    "        transcription = re.sub(r'[\\\\]', '', transcription)\n",
    "        transcription = re.sub(r'[/]', '', transcription)\n",
    "        transcription = re.sub(u'[Â¥Â£Â°Â¾Â½Â²]', '', transcription)\n",
    "        transcription = re.sub(u'[\\+><]', '', transcription)\n",
    "        return transcription\n",
    "\n",
    "    def _normalize_diacritics(transcription):\n",
    "        a = '[ÄÄƒÃ¡Ã£]'\n",
    "        u = '[Å«ÃºÃ¼]'\n",
    "        o = '[Ã¶ÃµÃ³]' \n",
    "        c = '[Ã§]'\n",
    "        i = '[Ã­]'\n",
    "        s = '[ÅŸ]'\n",
    "        e = '[Ã©]'\n",
    "\n",
    "        transcription = re.sub(a, \"a\", transcription)\n",
    "        transcription = re.sub(u, \"u\", transcription)\n",
    "        transcription = re.sub(o, \"o\", transcription)\n",
    "        transcription = re.sub(c, \"c\", transcription)\n",
    "        transcription = re.sub(i, \"i\", transcription)\n",
    "        transcription = re.sub(s, \"s\", transcription)\n",
    "        transcription = re.sub(e, \"e\", transcription)\n",
    "\n",
    "        return transcription\n",
    "\n",
    "    cleaned_transcriptions = map(_remove_special_characters, transcriptions)\n",
    "    cleaned_transcriptions = list(map(_normalize_diacritics, list(cleaned_transcriptions)))\n",
    "    return cleaned_transcriptions\n",
    "\n",
    "train_transcripts = preprocess_texts_hausa(train_transcripts)\n",
    "val_transcripts = preprocess_texts_hausa(val_transcripts)\n",
    "test_transcripts = preprocess_texts_hausa(test_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d341925a",
   "metadata": {},
   "source": [
    "Some models predict one character at a time, and so we need a character vocabulary made up of all characters in the dataset after preprocessing. We can save the vocabulary in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aaddb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def extract_all_chars(transcription):\n",
    "      all_text = \" \".join(transcription)\n",
    "      vocab = list(set(all_text))\n",
    "      return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocab_train = list(map(extract_all_chars, train_transcripts))\n",
    "vocab_val = list(map(extract_all_chars, val_transcripts))\n",
    "vocab_test = list(map(extract_all_chars, test_transcripts))\n",
    "\n",
    "vocab_train_chars = []\n",
    "for elem in [elem[\"vocab\"][0] for elem in vocab_train]:\n",
    "    vocab_train_chars.extend(elem)\n",
    "\n",
    "vocab_val_chars = []\n",
    "for elem in [elem[\"vocab\"][0] for elem in vocab_val]:\n",
    "    vocab_val_chars.extend(elem)\n",
    "\n",
    "vocab_test_chars = []\n",
    "for elem in [elem[\"vocab\"][0] for elem in vocab_test]:\n",
    "    vocab_test_chars.extend(elem)\n",
    "\n",
    "vocab_list = list(set(vocab_train_chars) | set(vocab_val_chars) | set(vocab_test_chars))\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "\n",
    "# for word delimiter, change \" \" --> \"|\" (ex. \"Hello my name is Bob\" --> \"Hello|my|name|is|Bob\")\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict) # this is for CTC to predict the end of a character (e.g. \"hhh[PAD]iii[PAD]iii[PAD]\" == \"hii\")\n",
    "\n",
    "with open(out_dir+\"vocab_hausa.json\", 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74417458",
   "metadata": {},
   "source": [
    "## Evaluation code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b7c289",
   "metadata": {},
   "source": [
    "In ASR, word error rate (WER) and character error rate (CER) are the common metrics used to evaluate how good a model-produced transcript is in comparison to the gold transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aef73068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14423/2987210431.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric, load_dataset, Audio\n",
    "wer_metric = load_metric(\"wer\")\n",
    "cer_metric = load_metric(\"cer\")\n",
    "                         \n",
    "def compute_metrics(label_strs, pred_strs):\n",
    "    # make sure labels are preprocessed the same way for proper comparison with other models after finetuning\n",
    "    wer = wer_metric.compute(predictions=pred_strs, references=label_strs) * 100\n",
    "    cer = cer_metric.compute(predictions=pred_strs, references=label_strs) * 100\n",
    "    return {\"wer\": wer, \"cer\": cer}\n",
    "\n",
    "def compute_metrics_finetuning(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str) * 100\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str) * 100\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c88b179",
   "metadata": {},
   "source": [
    "## Section A: Zero-Shot ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc9422",
   "metadata": {},
   "source": [
    "Let's run inference on our audio dataset with different existing, ready-to-use pretrained and finetuned models. Later, when we compare model performance, we will compare performance on the test set since some models will be trained on the train split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec099d",
   "metadata": {},
   "source": [
    "### OpenAI Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72baf2",
   "metadata": {},
   "source": [
    "OpenAI's Whisper model is a pretrained encoder-decoder model that supports a set of languages without futher fine-tuning. Here, we will use whisper-small for example reasons. You can use the larger checkpoints for better performance, all which can be found on Hugging Face Hub: https://huggingface.co/openai/whisper-small.\n",
    "\n",
    "Note: Whisper requires that input is sampled at 16,000 Hz. FLEURS data is sampled at this rate, so we are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "422ba1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took 13.261895656585693 seconds\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 30.18 GiB (GPU 0; 79.20 GiB total capacity; 39.08 GiB already allocated; 15.90 GiB free; 42.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThat took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# generate token ids\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m predicted_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_decoder_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforced_decoder_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThat took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1664\u001b[0m, in \u001b[0;36mWhisperForConditionalGeneration.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mreturn_timestamps:\n\u001b[1;32m   1662\u001b[0m     logits_processor \u001b[38;5;241m=\u001b[39m [WhisperTimeStampLogitsProcessor(generation_config)]\n\u001b[0;32m-> 1664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/transformers/generation/utils.py:1329\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1322\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1323\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration results, please set `padding_side=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` when initializing the tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1324\u001b[0m         )\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m     \u001b[38;5;66;03m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1329\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/transformers/generation/utils.py:642\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    640\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    641\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 642\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:863\u001b[0m, in \u001b[0;36mWhisperEncoder.forward\u001b[0;34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    856\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    857\u001b[0m             create_custom_forward(encoder_layer),\n\u001b[1;32m    858\u001b[0m             hidden_states,\n\u001b[1;32m    859\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    860\u001b[0m             (head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    861\u001b[0m         )\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 863\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:431\u001b[0m, in \u001b[0;36mWhisperEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    429\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    430\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[0;32m--> 431\u001b[0m hidden_states, attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    438\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:350\u001b[0m, in \u001b[0;36mWhisperAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    347\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len) \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    348\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len)\n\u001b[0;32m--> 350\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer_head_mask\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,):\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/torch/nn/functional.py:1843\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1845\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 30.18 GiB (GPU 0; 79.20 GiB total capacity; 39.08 GiB already allocated; 15.90 GiB free; 42.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "batch_size = 200\n",
    "device = \"cuda:0\" # change this to a gpu if you have access to one, otherwise set to \"cpu\"\n",
    "model_id = \"openai/whisper-small\"\n",
    "processor = WhisperProcessor.from_pretrained(model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id).to(device)\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"hausa\", task=\"transcribe\")\n",
    "\n",
    "predicted_test_transcripts = []\n",
    "\n",
    "for i in range(0, len(test_audio)/batch_size + len(test_audio)%batch_size, batch_size):\n",
    "    \n",
    "input_features = processor(test_audio[:batch_size], sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "# decode token ids to text\n",
    "predicted_test_transcripts = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predicted_test_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73753c9b",
   "metadata": {},
   "source": [
    "Let's evaluate the performance of whisper-small on our preprocessed test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521fe204",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(test_transcripts[:15], predicted_test_transcripts[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe8e1a",
   "metadata": {},
   "source": [
    "It looks like we have a 102.6% WER and 42.2% CER. Let's create a running table of the performances of different models on our test dataset. \n",
    "\n",
    "| Model | WER | CER|\n",
    "|-------|-----|----|\n",
    "|whisper-small| 102.6| 42.2|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a2a508",
   "metadata": {},
   "source": [
    "### Facebook MMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c219b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59a6325a",
   "metadata": {},
   "source": [
    "## Section B: Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1358da3f",
   "metadata": {},
   "source": [
    "### Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4357c4c",
   "metadata": {},
   "source": [
    "### MMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e91f3a",
   "metadata": {},
   "source": [
    "### XLS-R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ada4a",
   "metadata": {},
   "source": [
    "## Section C: Further Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153feed",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3831d",
   "metadata": {},
   "source": [
    "### Adding more data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

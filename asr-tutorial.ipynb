{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ea45ff",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition (ASR) Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b82bb",
   "metadata": {},
   "source": [
    "## Fine-tune a pretrained, multilingual ASR model on FLEURS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0545f5",
   "metadata": {},
   "source": [
    "In this tutorial, we will be evaluating and improving a multilingual ASR model for a language in the FLEURS dataset. We will focus on **Hausa**, but you can follow along in any language in FLEURS. See the FLEURS dataset paper for a list of supported languages: https://arxiv.org/abs/2205.12446.\n",
    "\n",
    "We will be looking at three major open-source ASR multilingual models:\n",
    "* XLS-R  (https://arxiv.org/abs/2111.09296)\n",
    "* Whisper (https://cdn.openai.com/papers/whisper.pdf)\n",
    "* MMS (https://scontent-sjc3-1.xx.fbcdn.net/v/t39.8562-6/348827959_6967534189927933_6819186233244071998_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=-JOSFMsFL-UAX-4O6o4&_nc_ht=scontent-sjc3-1.xx&oh=00_AfDdMFq0DP2xIRyjWpGrmIpqncnouiylLfWnFsAgxboLWw&oe=6497E242)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945531e",
   "metadata": {},
   "source": [
    "## Before you start: Setting up your coding environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7e5c2",
   "metadata": {},
   "source": [
    "You can run follow along and run the lines of code in this notebook, and also utilize the scripts found in this GitHub respository. Before starting this tutorial, you will need to create a virtual environment for this project so you can download all the required packages without affecting your other projects. We recommend using Anaconda (conda) to create a virtual environment. We have provided an `environment.yml` file that you can use to create a virtual environment named `asr` containing all the required packages. In your terminal run this code:\n",
    "\n",
    "```\n",
    "git clone https://github.com/kashrest/lrl-asr-experiments.git\n",
    "cd lrl-asr-experiments\n",
    "conda env create -f environment.yml \n",
    "conda activate asr\n",
    "```\n",
    "\n",
    "Now you can run the lines in this notebook.\n",
    "\n",
    "Note: The pretrained multilingual ASR models we will be using in this notebook require GPUs with at least 40 GB of space (CHECK) for practical use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71af9f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbbef6",
   "metadata": {},
   "source": [
    "The first step is to download and prepare the data for the ASR model. Hugging Face has an easy way to download FLEURS data for any supported language, where the split can be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24889296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fleurs (/data/users/kashrest/lrl-asr-experiments/data/fleurs/google___fleurs/ha_ng/2.0.0/af82dbec419a815084fa63ebd5d5a9f24a6e9acdf9887b9e3b8c6bbd64e0b7ac)\n",
      "Found cached dataset fleurs (/data/users/kashrest/lrl-asr-experiments/data/fleurs/google___fleurs/ha_ng/2.0.0/af82dbec419a815084fa63ebd5d5a9f24a6e9acdf9887b9e3b8c6bbd64e0b7ac)\n",
      "Found cached dataset fleurs (/data/users/kashrest/lrl-asr-experiments/data/fleurs/google___fleurs/ha_ng/2.0.0/af82dbec419a815084fa63ebd5d5a9f24a6e9acdf9887b9e3b8c6bbd64e0b7ac)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir_fleurs = \"./data/fleurs/\"\n",
    "\n",
    "# create a data directory for caching \n",
    "try:\n",
    "    os.mkdir(cache_dir_fleurs)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# create a directory for outputs \n",
    "out_dir = \"./tutorial/\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(out_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# for Hausa, the language code is \"ha_ng\"\n",
    "train_data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"train\", cache_dir=cache_dir_fleurs)\n",
    "val_data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"validation\", cache_dir=cache_dir_fleurs)\n",
    "test_data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"test\", cache_dir=cache_dir_fleurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8752ed3a",
   "metadata": {},
   "source": [
    "FLEURS data is organized like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab66092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 302,\n",
       " 'num_samples': 301440,\n",
       " 'path': '/data/users/kashrest/asr-experiments/downloads/extracted/953575415c600d2042020b042380119265aefaa4fcf95afc300adfcd2b79784e/10002175198254707815.wav',\n",
       " 'audio': {'path': 'train/10002175198254707815.wav',\n",
       "  'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         6.78300858e-05, 1.26361847e-05, 6.46114349e-05]),\n",
       "  'sampling_rate': 16000},\n",
       " 'transcription': 'nasarorin da vautier ta samu marasa alaka da bada umarni ba sun hada da yajin cin abinci a 1973 a kan abin da ya ke ganin dabaibayin siyasa ne',\n",
       " 'raw_transcription': 'Nasarorin da Vautier ta samu marasa alaka da bada umarni ba sun hada da yajin cin abinci a 1973 a kan abin da ya ke ganin dabaibayin siyasa ne.',\n",
       " 'gender': 1,\n",
       " 'lang_id': 30,\n",
       " 'language': 'Hausa',\n",
       " 'lang_group_id': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec16d08d",
   "metadata": {},
   "source": [
    "We are interested in the audio (represented as an array of floats each describing the amplitude/loudness of the sound; the number of floats is determined by the sampling rate which here is 16,000 measurements per second) and the corresponding transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d6e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transcripts, val_transcripts, test_transcripts = [], [], []\n",
    "train_audio, val_audio, test_audio = [], [], []\n",
    "\n",
    "for elem in train_data:\n",
    "    assert elem[\"audio\"][\"sampling_rate\"] == 16000\n",
    "    train_audio.append(elem[\"audio\"][\"array\"])\n",
    "    train_transcripts.append(elem[\"raw_transcription\"])\n",
    "    \n",
    "for elem in val_data:\n",
    "    assert elem[\"audio\"][\"sampling_rate\"] == 16000\n",
    "    val_audio.append(elem[\"audio\"][\"array\"])\n",
    "    val_transcripts.append(elem[\"raw_transcription\"])\n",
    "    \n",
    "for elem in test_data:\n",
    "    assert elem[\"audio\"][\"sampling_rate\"] == 16000\n",
    "    test_audio.append(elem[\"audio\"][\"array\"])\n",
    "    test_transcripts.append(elem[\"raw_transcription\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945c231",
   "metadata": {},
   "source": [
    "Now, since we are interested in transcribing speech, we want to clean the transcripts by removing special characters that do not have a clear sound (such as ! '). This part may depend on your target application and language. For example for Hausa, many native speakers do not speak English and does not have much code-switching, so we want to also normalize any foreign characters (Ã§ ÅŸ) and symbols (% & $)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b34939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_texts_hausa(transcriptions):\n",
    "    chars_to_remove_regex = '[\\,\\?\\!\\-\\;\\:\\\"\\â€œ\\%\\â€˜\\'\\Ê»\\â€\\ï¿½\\$\\&\\(\\)\\â€“\\â€”]'\n",
    "\n",
    "    def _remove_special_characters(transcription):\n",
    "        transcription = transcription.strip()\n",
    "        transcription = transcription.lower()\n",
    "        transcription = re.sub(chars_to_remove_regex, '', transcription)\n",
    "        transcription = re.sub(\"\\[\\]\\{\\}\", '', transcription)\n",
    "        transcription = re.sub(r'[\\\\]', '', transcription)\n",
    "        transcription = re.sub(r'[/]', '', transcription)\n",
    "        transcription = re.sub(u'[Â¥Â£Â°Â¾Â½Â²]', '', transcription)\n",
    "        transcription = re.sub(u'[\\+><]', '', transcription)\n",
    "        return transcription\n",
    "\n",
    "    def _normalize_diacritics(transcription):\n",
    "        a = '[ÄÄƒÃ¡Ã£]'\n",
    "        u = '[Å«ÃºÃ¼]'\n",
    "        o = '[Ã¶ÃµÃ³]' \n",
    "        c = '[Ã§]'\n",
    "        i = '[Ã­]'\n",
    "        s = '[ÅŸ]'\n",
    "        e = '[Ã©]'\n",
    "\n",
    "        transcription = re.sub(a, \"a\", transcription)\n",
    "        transcription = re.sub(u, \"u\", transcription)\n",
    "        transcription = re.sub(o, \"o\", transcription)\n",
    "        transcription = re.sub(c, \"c\", transcription)\n",
    "        transcription = re.sub(i, \"i\", transcription)\n",
    "        transcription = re.sub(s, \"s\", transcription)\n",
    "        transcription = re.sub(e, \"e\", transcription)\n",
    "\n",
    "        return transcription\n",
    "\n",
    "    cleaned_transcriptions = map(_remove_special_characters, transcriptions)\n",
    "    cleaned_transcriptions = list(map(_normalize_diacritics, list(cleaned_transcriptions)))\n",
    "    return cleaned_transcriptions\n",
    "\n",
    "train_transcripts = preprocess_texts_hausa(train_transcripts)\n",
    "val_transcripts = preprocess_texts_hausa(val_transcripts)\n",
    "test_transcripts = preprocess_texts_hausa(test_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d341925a",
   "metadata": {},
   "source": [
    "Some models predict one character at a time, and so we need a character vocabulary made up of all characters in the dataset after preprocessing. We can save the vocabulary in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f34513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def extract_all_chars(transcription):\n",
    "      all_text = \" \".join(transcription)\n",
    "      vocab = list(set(all_text))\n",
    "      return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocab_train = list(map(extract_all_chars, train_transcripts))\n",
    "vocab_val = list(map(extract_all_chars, val_transcripts))\n",
    "vocab_test = list(map(extract_all_chars, test_transcripts))\n",
    "\n",
    "vocab_train_chars = []\n",
    "for elem in [elem[\"vocab\"][0] for elem in vocab_train]:\n",
    "    vocab_train_chars.extend(elem)\n",
    "\n",
    "vocab_val_chars = []\n",
    "for elem in [elem[\"vocab\"][0] for elem in vocab_val]:\n",
    "    vocab_val_chars.extend(elem)\n",
    "\n",
    "vocab_test_chars = []\n",
    "for elem in [elem[\"vocab\"][0] for elem in vocab_test]:\n",
    "    vocab_test_chars.extend(elem)\n",
    "\n",
    "vocab_list = list(set(vocab_train_chars) | set(vocab_val_chars) | set(vocab_test_chars))\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "\n",
    "# for word delimiter, change \" \" --> \"|\" (ex. \"Hello my name is Bob\" --> \"Hello|my|name|is|Bob\")\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict) # this is for models (like MMS and XLS-R) that use the CTC algorithm to predict the end of a character (e.g. \"hhh[PAD]iii[PAD]iii[PAD]\" == \"hii\")\n",
    "\n",
    "vocab_file = out_dir+\"vocab_hausa.json\"\n",
    "with open(vocab_file, 'w') as f:\n",
    "    json.dump(vocab_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74417458",
   "metadata": {},
   "source": [
    "## Evaluation code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b7c289",
   "metadata": {},
   "source": [
    "In ASR, word error rate (WER) and character error rate (CER) are the common metrics used to evaluate how good a model-produced transcript is in comparison to the gold transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "117f2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric, load_dataset, Audio\n",
    "                         \n",
    "def compute_metrics(label_strs, pred_strs):\n",
    "    wer_metric = load_metric(\"wer\")\n",
    "    cer_metric = load_metric(\"cer\")\n",
    "    # make sure labels are preprocessed the same way for proper comparison with other models after finetuning\n",
    "    wer = wer_metric.compute(predictions=pred_strs, references=label_strs) * 100\n",
    "    cer = cer_metric.compute(predictions=pred_strs, references=label_strs) * 100\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c88b179",
   "metadata": {},
   "source": [
    "## Section A: Zero-Shot ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc9422",
   "metadata": {},
   "source": [
    "Let's run inference on our audio dataset with different existing, ready-to-use pretrained and finetuned models. Later, when we compare model performance, we will compare performance on the test set since some models will be trained on the train split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec099d",
   "metadata": {},
   "source": [
    "### OpenAI Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72baf2",
   "metadata": {},
   "source": [
    "OpenAI's Whisper model is a pretrained encoder-decoder model that supports a set of languages without futher fine-tuning. Here, we will use whisper-large V2 to compare it with MMS-1b-all (both have about 1 billion parameters). You can use the smaller checkpoints if you do not have enough GPU space (found on Hugging Face Hub: https://huggingface.co/openai/whisper-small) or decrease the batch size.\n",
    "\n",
    "Note: Whisper requires that input is sampled at 16,000 Hz. FLEURS data is sampled at this rate, so we are good. Also, Whisper does not support all FLEURS languages (the 96 languages are listed in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23266ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e2665",
   "metadata": {},
   "source": [
    "With a batch size of 10, inference takes about 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "422ba1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/63 [00:00<?, ?it/s]/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [05:19<00:00,  5.07s/it]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" # change this to a gpu if you have access to one, otherwise set to \"cpu\"\n",
    "model_id = \"openai/whisper-large-v2\" # 8630MiB for batch size 1, ~25426MiB for batch size 10\n",
    "processor = WhisperProcessor.from_pretrained(model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id).to(device)\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"hausa\", task=\"transcribe\")\n",
    "\n",
    "predicted_test_transcripts = []\n",
    "\n",
    "batch_size = 10 # decrease if needed\n",
    "\n",
    "for i in tqdm(range(0, len(test_audio), batch_size)):\n",
    "    batch = test_audio[i:i+batch_size] if i+batch_size <= len(test_audio) else test_audio[i:]\n",
    "    input_features = processor(batch, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "    # generate token ids\n",
    "    predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "    # decode token ids to text\n",
    "    predicted_test_transcripts.extend(processor.batch_decode(predicted_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73753c9b",
   "metadata": {},
   "source": [
    "Let's evaluate the performance of whisper-large V2 on our preprocessed test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "521fe204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19072/680328526.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'wer': 97.78959507944643, 'cer': 40.52395399540877}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(test_transcripts, predicted_test_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe8e1a",
   "metadata": {},
   "source": [
    "It looks like we have a 97.8% WER and 40.5% CER. Let's create a running table of the performances of different models on our test dataset. \n",
    "\n",
    "| Model | WER % | CER %|\n",
    "|-------|-----|----|\n",
    "|whisper-large-v2| 97.8| 40.5|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a2a508",
   "metadata": {},
   "source": [
    "### Facebook MMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7909c2",
   "metadata": {},
   "source": [
    "A batch size of 10 takes approximately 1 hour (?!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7db55c0c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–ˆâ–Š                                                                             | 14/621 [00:35<25:38,  2.53s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(elem, sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16_000\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 27\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     28\u001b[0m ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m predicted_test_transcripts\u001b[38;5;241m.\u001b[39mappend(processor\u001b[38;5;241m.\u001b[39mdecode(ids))\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1939\u001b[0m, in \u001b[0;36mWav2Vec2ForCTC.forward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   1929\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1930\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;124;03m    Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1934\u001b[0m \u001b[38;5;124;03m    config.vocab_size - 1]`.\u001b[39;00m\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1939\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwav2vec2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1945\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1947\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1948\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1560\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1555\u001b[0m hidden_states, extract_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_projection(extract_features)\n\u001b[1;32m   1556\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_hidden_states(\n\u001b[1;32m   1557\u001b[0m     hidden_states, mask_time_indices\u001b[38;5;241m=\u001b[39mmask_time_indices, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask\n\u001b[1;32m   1558\u001b[0m )\n\u001b[0;32m-> 1560\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1568\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:908\u001b[0m, in \u001b[0;36mWav2Vec2EncoderStableLayerNorm.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    902\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    903\u001b[0m             create_custom_forward(layer),\n\u001b[1;32m    904\u001b[0m             hidden_states,\n\u001b[1;32m    905\u001b[0m             attention_mask,\n\u001b[1;32m    906\u001b[0m         )\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 908\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_the_layer:\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:737\u001b[0m, in \u001b[0;36mWav2Vec2EncoderLayerStableLayerNorm.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    735\u001b[0m attn_residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    736\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 737\u001b[0m hidden_states, attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    741\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn_residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:607\u001b[0m, in \u001b[0;36mWav2Vec2Attention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    604\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention mask should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;250m \u001b[39mtgt_len,\u001b[38;5;250m \u001b[39msrc_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattention_mask\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         )\n\u001b[1;32m    606\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len) \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[0;32m--> 607\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attn_weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, AutoProcessor\n",
    "\n",
    "model_id = \"facebook/mms-1b-all\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
    "\n",
    "processor.tokenizer.set_target_lang(\"hau\")\n",
    "model.load_adapter(\"hau\")\n",
    "\n",
    "predicted_test_transcripts = []\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "\"\"\"for i in tqdm(range(0, len(test_audio), batch_size)):\n",
    "    batch = test_audio[i:i+batch_size] if i+batch_size <= len(test_audio) else test_audio[i:]\n",
    "    inputs = processor(batch, sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "    ids = torch.argmax(outputs, dim=-1)\n",
    "    predicted_test_transcripts.extend((processor.batch_decode(ids)))\n",
    "\"\"\"\n",
    "for elem in tqdm(test_audio): \n",
    "    inputs = processor(elem, sampling_rate=16_000, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "    ids = torch.argmax(outputs, dim=-1)[0]\n",
    "    predicted_test_transcripts.append(processor.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9868c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(test_transcripts, predicted_test_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4874ff",
   "metadata": {},
   "source": [
    "It looks like we have a 29.3% WER and 7.7% CER. Let's add this result to our table. This is substantially better than whisper-large!\n",
    "\n",
    "| Model | WER % | CER %|\n",
    "|-------|-----|----|\n",
    "|whisper-large-v2| 97.8| 40.5|\n",
    "|mms-1b-all|29.3|7.7|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6325a",
   "metadata": {},
   "source": [
    "## Section B: Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aacf6e2",
   "metadata": {},
   "source": [
    "For fine-tuning, we will be using functions from the Hugging Face API for the training loop and model setup. In order to use these functions, we need to wrap the data in a custom PyTorch Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c1e5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class ASRDatasetWav2Vec2(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio, transcripts, sampling_rate, processor):\n",
    "        self.audio = audio\n",
    "        self.transcripts = transcripts\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_values = self.processor.feature_extractor(self.audio[idx], sampling_rate=self.sampling_rate).input_values[0]\n",
    "        labels = self.processor.tokenizer(self.transcripts[idx]).input_ids\n",
    "        item = {}\n",
    "        item[\"input_values\"] = input_values\n",
    "        item[\"labels\"] = labels\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1f79039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRDatasetWhisper(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio, transcripts, sampling_rate, processor):\n",
    "        self.audio = audio\n",
    "        self.transcripts = transcripts\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_values = self.processor.feature_extractor(self.audio[idx], sampling_rate=self.sampling_rate).input_features[0]\n",
    "        labels = self.processor.tokenizer(self.transcripts[idx]).input_ids\n",
    "        item = {}\n",
    "        item[\"input_features\"] = input_values\n",
    "        item[\"labels\"] = labels\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1358da3f",
   "metadata": {},
   "source": [
    "### Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b2a9a4",
   "metadata": {},
   "source": [
    "First, let's import the required Whisper classes and training loop functions from Hugging Face and some other utility functions\n",
    "\n",
    "Note: Hugging Face has a great tutorial that we referenced for fine-tuning Whisper. You can refer to this tutorial for more information if needed: https://huggingface.co/blog/fine-tune-whisper#prepare-feature-extractor-tokenizer-and-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd7b8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca4305",
   "metadata": {},
   "source": [
    "Let's fine-tune a Whisper-large V2 model which we will download from Hugging Face, and set up a WhisperProcessor object which takes contains a feature extractor and a tokenizer. The feature extractor transforms the input into log-Mel spectrograms. This transformation takes in the amplitude information respresented by the input array and transforms it into frequencies (refer to the Hugging Face tutorial for more information). Frequencies encode pitch, and so useful audio signals can be found for speech recognition. Additionally, the tokenizer splits the transcripts into tokens based on Whisper's vocabulary. Whisper utilizes byte-level BPE, which is the same tokenizer as GPT-2. If interested, refer to this page: https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cab8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = \"openai/whisper-large-v2\"\n",
    "processor = WhisperProcessor.from_pretrained(model_card, language=\"Hausa\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aec1c5",
   "metadata": {},
   "source": [
    "The following lines are required to fine-tune the Whisper model. The first line makes the model predict the language and task by setting the token ids that control the transcription language and task, to `None`.\n",
    "\n",
    "The second line makes sure that all possible tokens are predicted by setting the set of supressed tokens to an empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7657fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c83da",
   "metadata": {},
   "source": [
    "Like mentioned before, Whisper takes inputs sampled at 16,000 Hz, and so we will prepare our data using this sampling rate (Fleurs is already sampled at 16 kHz) using the ASRDataset object mentioned before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "806959ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sampling_rate = 16000\n",
    "train_dataset = ASRDatasetWhisper(train_audio, train_transcripts, model_sampling_rate, processor)\n",
    "val_dataset = ASRDatasetWhisper(val_audio,  val_transcripts, model_sampling_rate, processor)\n",
    "test_dataset = ASRDatasetWhisper(test_audio, test_transcripts, model_sampling_rate, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3fab6f",
   "metadata": {},
   "source": [
    "Next, we need a function that will pad all the inputs/outputs in a batch to the same length. This code is from the tutorial mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d1fdde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "    \n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fab3d7",
   "metadata": {},
   "source": [
    "In order to use the Trainer class from Hugging Face, we need to define an evaluation function that takes in a model prediction object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52d2304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    metric_wer = evaluate.load(\"wer\")\n",
    "    metric_cer = evaluate.load(\"cer\")\n",
    "    \n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric_wer.compute(predictions=pred_str, references=label_str)\n",
    "    cer = 100 * metric_cer.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b7c41f",
   "metadata": {},
   "source": [
    "Now, we will setup the model training hyperparameters by using Hugging Face Seq2SeqTrainingArguments. Feel free to experiment with different hyperparameters. Learning rate is an important hyperparameter to experiment with. Reference the official Seq2SeqTrainingArguments for explanations of the hyperparameters: https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments\n",
    "\n",
    "Note: Decrease batch size if you have limited GPU space. We have also set the mixed precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a77b5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16\n",
    "num_train_epochs = 3 \n",
    "learning_rate = 1e-05\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=out_dir+\"whisper-finetuning-experiment-2/\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=500,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    gradient_checkpointing=True, # another way to save GPU memory by recomputing gradients (less memory, more time)\n",
    "    fp16=True, # this enables mixed precision training, which lets some data be stored in 16 bit floating point precision instead of 32 bits.\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2, \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada10041",
   "metadata": {},
   "source": [
    "Now, we set up the Trainer object by inputing our training and validation datasets, our evaluation function, tokenizer, model, data collator, and previously instantiated training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47ee3809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e673df",
   "metadata": {},
   "source": [
    "Then we call train to start training. Training the Whisper large V2 model with batch size 16 for 3 epochs takes about 1 hour and about 56846MiB (57 GB) on a NVIDIA A100 GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "577fd897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='612' max='612' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [612/612 1:04:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.678611</td>\n",
       "      <td>47.085015</td>\n",
       "      <td>15.846247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.626455</td>\n",
       "      <td>53.026297</td>\n",
       "      <td>22.505824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.603841</td>\n",
       "      <td>48.699040</td>\n",
       "      <td>20.497822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.572886</td>\n",
       "      <td>42.646445</td>\n",
       "      <td>18.659475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.596419</td>\n",
       "      <td>53.902880</td>\n",
       "      <td>24.265674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.549716</td>\n",
       "      <td>41.185474</td>\n",
       "      <td>18.540464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished in 3869.6817395687103 seconds.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4963a5c6",
   "metadata": {},
   "source": [
    "Let's see the performance on the FLEURS test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d02878c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'wer': 40.395950794464376, 'cer': 19.22895013093082}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = trainer.predict(test_dataset)\n",
    "eval_preds = compute_metrics(preds)\n",
    "eval_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35718d6e",
   "metadata": {},
   "source": [
    "It looks like we have a 40.4% WER and 19.2% CER. Great! We have some made some improvement after finetuning for 3 epochs. Let's add this result to our table. \n",
    "\n",
    "| Model | WER % | CER %|\n",
    "|-------|-----|----|\n",
    "|whisper-large-v2| 97.8| 40.5|\n",
    "|mms-1b-all|**29.3**|**7.7**|\n",
    "|finetuned whisper-large-v2|40.4|19.2|\n",
    "\n",
    "It looks like mms-1b-all still has the best results. Let's see if further finetuning mms-1b-all is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4357c4c",
   "metadata": {},
   "source": [
    "### MMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13210611",
   "metadata": {},
   "source": [
    "MMS-1b-all is Facebook's MMS (**M**assively **M**ultilingual **S**peech) model, which is MMS, an ASR (Wav2Vec) model that is pretrained on a large corpus of Bible data covering 1107 languages, finetuned on additional labeled datasets. We wil further fine-tune MMS to see if it can be improved by further finetuning on our FLEURS dataset. You can refer to Hugging Face's recent MMS finetuning blog for more details and explanations if needed: https://huggingface.co/blog/mms_adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf5535",
   "metadata": {},
   "source": [
    "MMS-1b-all works by incorporating an adapter architecture, which are extra parameters throughout the architecture that are trainable during finetuning, and are language-specific. This enables the user to finetune a smaller number of parameters in comparison to the entire model.\n",
    "\n",
    "Here, we will finetune the MMS adapter weights for Hausa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d2424f",
   "metadata": {},
   "source": [
    "First, we will set up the tokenizer based on our previously made character vocabulary, setting special tokens for unknown characters, padding, and word delimiters according to the vocabulary. We need to specify our vocabulary for the specific language of interest in a dictionary so that the MMS-1b-all checkpoint will correctly finetune the adapter weights for Hausa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e1c1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "\n",
    "target_lang = \"hau\"\n",
    "\n",
    "with open(vocab_file, \"r\") as f:\n",
    "    vocab_dict = json.load(f)\n",
    "\n",
    "new_vocab_dict = {target_lang: vocab_dict}\n",
    "\n",
    "experiment_file = out_dir+\"mms-1b-all-finetuning-1/\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(experiment_file)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with open(experiment_file+\"vocab.json\", 'w') as f:\n",
    "    json.dump(new_vocab_dict, f)\n",
    "    \n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(experiment_file, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\", target_lang=target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96903bd0",
   "metadata": {},
   "source": [
    "Then, we will setup the feature extractor, which transforms the input audio into features. MMS takes in the raw audio, unlike the Whisper model, and simply zero-mean-unit-variance normalizes the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d755360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sampling_rate = 16000\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f9b27",
   "metadata": {},
   "source": [
    "And finally, the processor wraps both the tokenizer and feature extractor into one conventient class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60fe16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582b746d",
   "metadata": {},
   "source": [
    "Now, we want to create a data collator (similar to the one we made for Whisper) that prepares the input in batches for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a6280929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPaddingWav2Vec2: # credit: https://huggingface.co/blog/mms_adapters\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.pad(\n",
    "            labels=label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "data_collator = DataCollatorCTCWithPaddingWav2Vec2(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df23e31b",
   "metadata": {},
   "source": [
    "Now we create an evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbc17ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(pred):\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "    cer_metric = evaluate.load(\"cer\")\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22618f1",
   "metadata": {},
   "source": [
    "Now, we can define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69136f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized because the shapes did not match:\n",
      "- lm_head.bias: found shape torch.Size([154]) in the checkpoint and torch.Size([50]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([154, 1280]) in the checkpoint and torch.Size([50, 1280]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model_card = \"facebook/mms-1b-all\"\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_card,\n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ce7ec",
   "metadata": {},
   "source": [
    "We re-initialize the adapter layers to prepare for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87198e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_adapter_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a695c7",
   "metadata": {},
   "source": [
    "Then we freeze all the parameters (learned from the pretraining and finetuning by the Meta team) except the adapter weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e9fa3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_base_model()\n",
    "\n",
    "adapter_weights = model._get_adapters()\n",
    "for param in adapter_weights.values():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6256f550",
   "metadata": {},
   "source": [
    "Then, we set up the parameters for model training like for Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18563719",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16 # batch size 2 already takes 32472MiB (32 GB), batch size 16 takes 74049MiB (74 GB)\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48665d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=experiment_file,\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=train_batch_size,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=num_epochs,\n",
    "  gradient_checkpointing=True, # another way to save GPU memory by recomputing gradients (less memory, more time)\n",
    "  fp16=True, # this enables mixed precision training, which lets some data be stored in 16 bit floating point precision instead of 32 bits.\n",
    "  save_steps=200,\n",
    "  eval_steps=100,\n",
    "  logging_steps=100,\n",
    "  learning_rate=learning_rate,\n",
    "  warmup_steps=100,\n",
    "  save_total_limit=2,\n",
    "  push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf38c93a",
   "metadata": {},
   "source": [
    "Then send everything to the Trainer class for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "702443fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since our processor is different, we will need to create new ASRDataset objects\n",
    "train_dataset = ASRDatasetWav2Vec2(train_audio, train_transcripts, model_sampling_rate, processor)\n",
    "val_dataset = ASRDatasetWav2Vec2(val_audio,  val_transcripts, model_sampling_rate, processor)\n",
    "test_dataset = ASRDatasetWav2Vec2(test_audio, test_transcripts, model_sampling_rate, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86677411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cdbef5",
   "metadata": {},
   "source": [
    "Training 3 epochs with batch size 16 takes about 25 minutes and about 74049MiB (74 GB) on an NVIDIA A100 GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c2e53e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='612' max='612' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [612/612 23:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>0.253368</td>\n",
       "      <td>0.287324</td>\n",
       "      <td>0.074749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.207100</td>\n",
       "      <td>0.245935</td>\n",
       "      <td>0.288298</td>\n",
       "      <td>0.073331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.198300</td>\n",
       "      <td>0.248379</td>\n",
       "      <td>0.297064</td>\n",
       "      <td>0.083004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.198400</td>\n",
       "      <td>0.253414</td>\n",
       "      <td>0.286907</td>\n",
       "      <td>0.075104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.185100</td>\n",
       "      <td>0.249626</td>\n",
       "      <td>0.285933</td>\n",
       "      <td>0.082928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>0.249387</td>\n",
       "      <td>0.282594</td>\n",
       "      <td>0.079864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=612, training_loss=0.19829010495952532, metrics={'train_runtime': 1454.1031, 'train_samples_per_second': 6.724, 'train_steps_per_second': 0.421, 'total_flos': 1.4198738598624823e+19, 'train_loss': 0.19829010495952532, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "206f189d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'wer': 0.27905933615276174, 'cer': 0.08098286900847898}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = trainer.predict(test_dataset)\n",
    "eval_preds = compute_metrics(preds)\n",
    "eval_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0171c6f0",
   "metadata": {},
   "source": [
    "It looks like we have a 27.9% WER and 8.1% CER. Interesting, we have a 4% decrease in WER but a 5% increase in CER... We have some made some improvement in WER after finetuning. Perhaps a different set of hyperparameters (such as learning rate) would show better results. Please refer to Section C for guidance on how to experiment with different hyperparameters. Let's add this result to our table. \n",
    "\n",
    "| Model | WER % | CER %|\n",
    "|-------|-----|----|\n",
    "|whisper-large-v2| 97.8| 40.5|\n",
    "|mms-1b-all|29.3|**7.7**|\n",
    "|finetuned whisper-large-v2|40.4|19.2|\n",
    "|finetuned mms-1b-all| **27.9**| 8.1|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e91f3a",
   "metadata": {},
   "source": [
    "### XLS-R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba222f69",
   "metadata": {},
   "source": [
    "XLS-R was released before MMS, and the MMS paper claims (CHECK) that it has better performance than XLS-R. However, it may be a good idea to check to see which model is better for your specific dataset and use-case. Therefore, let's try finetuning XLS-R on the Hausa fleurs dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abe94e7",
   "metadata": {},
   "source": [
    "Similar to MMS, we will create a tokenizer from the character vocabulary file we made earlier in this tutorial, then the feature extractor and processor that wraps the tokenizer and feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a92ff727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "tokenizer = Wav2Vec2CTCTokenizer(vocab_file, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=model_sampling_rate, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7448a305",
   "metadata": {},
   "source": [
    "Then, we want to create our dataset objects using our processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2196635",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ASRDatasetWav2Vec2(train_audio, train_transcripts, model_sampling_rate, processor)\n",
    "val_dataset = ASRDatasetWav2Vec2(val_audio,  val_transcripts, model_sampling_rate, processor)\n",
    "test_dataset = ASRDatasetWav2Vec2(test_audio, test_transcripts, model_sampling_rate, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac62e05",
   "metadata": {},
   "source": [
    "Then, we want to instantiate a data collator of the same class as the one for MMS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55ff3797",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPaddingWav2Vec2(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ad047",
   "metadata": {},
   "source": [
    "Now, we create variables and functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ce098065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(pred):\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "    cer_metric = evaluate.load(\"cer\")\n",
    "    \n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b46f33ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-xls-r-1b were not used when initializing Wav2Vec2ForCTC: ['project_q.bias', 'project_q.weight', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-1b and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 2e-4\n",
    "num_train_epochs = 3\n",
    "attention_dropout = 0.1\n",
    "hidden_dropout = 0.1\n",
    "feat_proj_dropout = 0.0\n",
    "mask_time_prob = 0.05\n",
    "layerdrop = 0.1\n",
    "warmup_steps = 500\n",
    "    \n",
    "model_card = \"facebook/wav2vec2-xls-r-1b\"\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_card, \n",
    "    attention_dropout=attention_dropout,\n",
    "    hidden_dropout=hidden_dropout,\n",
    "    feat_proj_dropout=feat_proj_dropout,\n",
    "    mask_time_prob=mask_time_prob,\n",
    "    layerdrop=layerdrop,\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "model.freeze_feature_extractor()\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c1a36267",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=out_dir,\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=batch_size,\n",
    "  gradient_accumulation_steps=3,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=num_train_epochs,\n",
    "  fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  load_best_model_at_end=True,\n",
    "  learning_rate=learning_rate,\n",
    "  warmup_steps=warmup_steps,\n",
    "  save_total_limit=2,\n",
    "  metric_for_best_model=\"wer\",\n",
    "  greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "672d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset, \n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf3161",
   "metadata": {},
   "source": [
    "Training with batch size of 8 and 3 epochs takes about _ minutes and 61338MiB (61 GB) on an NVIDIA-SMI. This model should be trained longer, we found that the loss stabalizes after around 10 epochs, so train for 10+ epochs.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7697fe63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='408' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/408 09:18 < 12:24, 0.31 it/s, Epoch 1.29/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.012300</td>\n",
       "      <td>2.884076</td>\n",
       "      <td>0.965772</td>\n",
       "      <td>0.828750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b65c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(test_dataset)\n",
    "eval_preds = compute_metrics(preds)\n",
    "eval_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6038a",
   "metadata": {},
   "source": [
    "It looks like we have a _% WER and _% CER. Let's add this result to our table. \n",
    "\n",
    "| Model | WER % | CER %|\n",
    "|-------|-----|----|\n",
    "|whisper-large-v2| 97.8| 40.5|\n",
    "|mms-1b-all|29.3|7.7|\n",
    "|finetuned whisper-large-v2|40.4|19.2|\n",
    "|finetuned mms-1b-all|27.9|8.1|\n",
    "|finetuned xls-r-1b|_|_|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ada4a",
   "metadata": {},
   "source": [
    "## Section C: Further Improvements **(under construction)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f18ecc",
   "metadata": {},
   "source": [
    "### Available scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc8b74",
   "metadata": {},
   "source": [
    "For convenience, we have provided in this GitHub repo a finetuning script that enables the user to enter any FLUERS language, custom prepared dataset, and model training hyperparameters to do finetuning and evaluation all in one easy script (in progress)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153feed",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab900e4",
   "metadata": {},
   "source": [
    "Using the scripts available in this GitHub repo, you can run your own experiments with different hyperparameters to see what gives the best model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3831d",
   "metadata": {},
   "source": [
    "#### Adding more data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b95d37",
   "metadata": {},
   "source": [
    "In order to use a dataset other than FLEURS, you must make sure to set up a Python script that has a function called `create_dataset()`. It must return three ASRDataset objects for the training, validation, and test set. The ASRDataset is available in the `utilities.py` script.\n",
    "\n",
    "*Example custom dataset script:*\n",
    "\n",
    "```\n",
    "from utilities import ASRDataset \n",
    "def create_dataset() -> Tuple[ASRDataset]:\n",
    "    # your code\n",
    "    train_dataset = ASRDataset(audio_train, transcripts_train, sampling_rate, processor)\n",
    "    val_dataset = ASRDataset(audio_val, transcripts_val, sampling_rate, processor)\n",
    "    test_dataset = ASRDataset(audio_test, transcripts_test, sampling_rate, processor)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ea45ff",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition (ASR) Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b82bb",
   "metadata": {},
   "source": [
    "## Fine-tune a pretrained, multilingual ASR model on FLEURS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0545f5",
   "metadata": {},
   "source": [
    "In this tutorial, we will be evaluating and improving a multilingual ASR model for a language in the FLEURS dataset. We will focus on **Hausa**, but you can follow along in any language in FLEURS. See the FLEURS dataset paper for a list of supported languages: https://arxiv.org/abs/2205.12446.\n",
    "\n",
    "We will be looking at three major open-source ASR multilingual models:\n",
    "* XLS-R  (https://arxiv.org/abs/2111.09296)\n",
    "* Whisper (https://cdn.openai.com/papers/whisper.pdf)\n",
    "* MMS (https://scontent-sjc3-1.xx.fbcdn.net/v/t39.8562-6/348827959_6967534189927933_6819186233244071998_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=-JOSFMsFL-UAX-4O6o4&_nc_ht=scontent-sjc3-1.xx&oh=00_AfDdMFq0DP2xIRyjWpGrmIpqncnouiylLfWnFsAgxboLWw&oe=6497E242)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945531e",
   "metadata": {},
   "source": [
    "## Before you start: Setting up your coding environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7e5c2",
   "metadata": {},
   "source": [
    "You can run follow along and run the lines of code in this notebook, and also utilize the scripts found in this GitHub respository. Before starting this tutorial, you will need to create a virtual environment for this project so you can download all the required packages without affecting your other projects. We recommend using Anaconda (conda) to create a virtual environment. We have provided an `environment.yml` file that you can use to create a virtual environment named `asr` containing all the required packages. In your terminal run this code:\n",
    "\n",
    "```\n",
    "git clone https://github.com/kashrest/lrl-asr-experiments.git\n",
    "cd lrl-asr-experiments\n",
    "conda env create -f environment.yml \n",
    "conda activate asr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fa7f76",
   "metadata": {},
   "source": [
    "Note: The pretrained multilingual ASR models we will be using in this notebook require GPUs with at least 40 GB of space (check) for practical use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e13d8d",
   "metadata": {},
   "source": [
    "Now you can run the lines in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71af9f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbbef6",
   "metadata": {},
   "source": [
    "The first step is to download and prepare the data for the ASR model. Hugging Face has an easy way to download FLEURS data for any supported language, where the split can be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24889296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fleurs (/data/users/kashrest/lrl-asr-experiments/data/fleurs/google___fleurs/ha_ng/2.0.0/af82dbec419a815084fa63ebd5d5a9f24a6e9acdf9887b9e3b8c6bbd64e0b7ac)\n",
      "Found cached dataset fleurs (/data/users/kashrest/lrl-asr-experiments/data/fleurs/google___fleurs/ha_ng/2.0.0/af82dbec419a815084fa63ebd5d5a9f24a6e9acdf9887b9e3b8c6bbd64e0b7ac)\n",
      "Found cached dataset fleurs (/data/users/kashrest/lrl-asr-experiments/data/fleurs/google___fleurs/ha_ng/2.0.0/af82dbec419a815084fa63ebd5d5a9f24a6e9acdf9887b9e3b8c6bbd64e0b7ac)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir_fleurs = \"./data/fleurs/\"\n",
    "out_dir = \"./asr-tutorial-experiment-1/\"\n",
    "\n",
    "# create a data directory for caching \n",
    "try:\n",
    "    os.mkdir(cache_dir_fleurs)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# create a directory for outputs \n",
    "try:\n",
    "    os.mkdir(out_dir)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# for Hausa, the language code is \"ha_ng\"\n",
    "train_data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"train\", cache_dir=cache_dir_fleurs)\n",
    "val_data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"validation\", cache_dir=cache_dir_fleurs)\n",
    "test_data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"test\", cache_dir=cache_dir_fleurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8752ed3a",
   "metadata": {},
   "source": [
    "FLEURS data is organized like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab66092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 302,\n",
       " 'num_samples': 301440,\n",
       " 'path': '/data/users/kashrest/asr-experiments/downloads/extracted/953575415c600d2042020b042380119265aefaa4fcf95afc300adfcd2b79784e/10002175198254707815.wav',\n",
       " 'audio': {'path': 'train/10002175198254707815.wav',\n",
       "  'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         6.78300858e-05, 1.26361847e-05, 6.46114349e-05]),\n",
       "  'sampling_rate': 16000},\n",
       " 'transcription': 'nasarorin da vautier ta samu marasa alaka da bada umarni ba sun hada da yajin cin abinci a 1973 a kan abin da ya ke ganin dabaibayin siyasa ne',\n",
       " 'raw_transcription': 'Nasarorin da Vautier ta samu marasa alaka da bada umarni ba sun hada da yajin cin abinci a 1973 a kan abin da ya ke ganin dabaibayin siyasa ne.',\n",
       " 'gender': 1,\n",
       " 'lang_id': 30,\n",
       " 'language': 'Hausa',\n",
       " 'lang_group_id': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec16d08d",
   "metadata": {},
   "source": [
    "We are interested in the audio (represented as an array of floats each describing the amplitude/loudness of the sound; the number of floats is determined by the sampling rate which here is 16,000 measurements per second) and the corresponding transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d6e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transcripts, val_transcripts, test_transcripts = [], [], []\n",
    "train_audio, val_audio, test_audio = [], [], []\n",
    "\n",
    "for elem in train_data:\n",
    "    train_audio.append(elem[\"audio\"][\"array\"])\n",
    "    train_transcripts.append(elem[\"raw_transcription\"])\n",
    "    \n",
    "for elem in val_data:\n",
    "    val_audio.append(elem[\"audio\"][\"array\"])\n",
    "    val_transcripts.append(elem[\"raw_transcription\"])\n",
    "    \n",
    "for elem in test_data:\n",
    "    test_audio.append(elem[\"audio\"][\"array\"])\n",
    "    test_transcripts.append(elem[\"raw_transcription\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945c231",
   "metadata": {},
   "source": [
    "Now, since we are interested in transcribing speech, we want to clean the transcripts by removing special characters that do not have a clear sound (such as ! '). This part may depend on your target application and language. For example for Hausa, many native speakers do not speak English and does not have much code-switching, so we want to also normalize any foreign characters (Ã§ ÅŸ) and symbols (% & $)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2158c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_texts_hausa(transcriptions):\n",
    "    chars_to_remove_regex = '[\\,\\?\\!\\-\\;\\:\\\"\\â€œ\\%\\â€˜\\'\\Ê»\\â€\\ï¿½\\$\\&\\(\\)\\â€“\\â€”]'\n",
    "\n",
    "    def _remove_special_characters(transcription):\n",
    "        transcription = transcription.strip()\n",
    "        transcription = transcription.lower()\n",
    "        transcription = re.sub(chars_to_remove_regex, '', transcription)\n",
    "        transcription = re.sub(\"\\[\\]\\{\\}\", '', transcription)\n",
    "        transcription = re.sub(r'[\\\\]', '', transcription)\n",
    "        transcription = re.sub(r'[/]', '', transcription)\n",
    "        transcription = re.sub(u'[Â¥Â£Â°Â¾Â½Â²]', '', transcription)\n",
    "        transcription = re.sub(u'[\\+><]', '', transcription)\n",
    "        return transcription\n",
    "\n",
    "    def _normalize_diacritics(transcription):\n",
    "        a = '[ÄÄƒÃ¡Ã£]'\n",
    "        u = '[Å«ÃºÃ¼]'\n",
    "        o = '[Ã¶ÃµÃ³]' \n",
    "        c = '[Ã§]'\n",
    "        i = '[Ã­]'\n",
    "        s = '[ÅŸ]'\n",
    "        e = '[Ã©]'\n",
    "\n",
    "        transcription = re.sub(a, \"a\", transcription)\n",
    "        transcription = re.sub(u, \"u\", transcription)\n",
    "        transcription = re.sub(o, \"o\", transcription)\n",
    "        transcription = re.sub(c, \"c\", transcription)\n",
    "        transcription = re.sub(i, \"i\", transcription)\n",
    "        transcription = re.sub(s, \"s\", transcription)\n",
    "        transcription = re.sub(e, \"e\", transcription)\n",
    "\n",
    "        return transcription\n",
    "\n",
    "    cleaned_transcriptions = map(_remove_special_characters, transcriptions)\n",
    "    cleaned_transcriptions = list(map(_normalize_diacritics, list(cleaned_transcriptions)))\n",
    "    return cleaned_transcriptions\n",
    "\n",
    "train_transcripts = preprocess_texts_hausa(train_transcripts)\n",
    "val_transcripts = preprocess_texts_hausa(val_transcripts)\n",
    "test_transcripts = preprocess_texts_hausa(test_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d341925a",
   "metadata": {},
   "source": [
    "Some models predict one character at a time, and so we need a character vocabulary made up of all characters in the dataset after preprocessing. We can save the vocabulary in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aaddb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def extract_all_chars(transcription):\n",
    "      all_text = \" \".join(transcription)\n",
    "      vocab = list(set(all_text))\n",
    "      return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocab_train = list(map(extract_all_chars, train_transcripts))\n",
    "vocab_val = list(map(extract_all_chars, val_transcripts))\n",
    "vocab_test = list(map(extract_all_chars, test_transcripts))\n",
    "\n",
    "vocab_train_chars = []\n",
    "for elem in [elem[\"vocab\"][0] for elem in vocab_train]:\n",
    "    vocab_train_chars.extend(elem)\n",
    "\n",
    "vocab_val_chars = []\n",
    "for elem in [elem[\"vocab\"][0] for elem in vocab_val]:\n",
    "    vocab_val_chars.extend(elem)\n",
    "\n",
    "vocab_test_chars = []\n",
    "for elem in [elem[\"vocab\"][0] for elem in vocab_test]:\n",
    "    vocab_test_chars.extend(elem)\n",
    "\n",
    "vocab_list = list(set(vocab_train_chars) | set(vocab_val_chars) | set(vocab_test_chars))\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "\n",
    "# for word delimiter, change \" \" --> \"|\" (ex. \"Hello my name is Bob\" --> \"Hello|my|name|is|Bob\")\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict) # this is for CTC to predict the end of a character (e.g. \"hhh[PAD]iii[PAD]iii[PAD]\" == \"hii\")\n",
    "\n",
    "with open(out_dir+\"vocab_hausa.json\", 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74417458",
   "metadata": {},
   "source": [
    "## Evaluation code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b7c289",
   "metadata": {},
   "source": [
    "In ASR, word error rate (WER) and character error rate (CER) are the common metrics used to evaluate how good a model-produced transcript is in comparison to the gold transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aef73068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6402/778497257.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric, load_dataset, Audio\n",
    "wer_metric = load_metric(\"wer\")\n",
    "cer_metric = load_metric(\"cer\")\n",
    "                         \n",
    "def compute_metrics(label_strs, pred_strs):\n",
    "    # make sure labels are preprocessed the same way for proper comparison with other models after finetuning\n",
    "    wer = wer_metric.compute(predictions=pred_strs, references=label_strs) * 100\n",
    "    cer = cer_metric.compute(predictions=pred_strs, references=label_strs) * 100\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c88b179",
   "metadata": {},
   "source": [
    "## Section A: Zero-Shot ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc9422",
   "metadata": {},
   "source": [
    "Let's run inference on our audio dataset with different existing, ready-to-use pretrained and finetuned models. Later, when we compare model performance, we will compare performance on the test set since some models will be trained on the train split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec099d",
   "metadata": {},
   "source": [
    "### OpenAI Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72baf2",
   "metadata": {},
   "source": [
    "OpenAI's Whisper model is a pretrained encoder-decoder model that supports a set of languages without futher fine-tuning. Here, we will use whisper-large V2 to compare it with MMS-1b-all (both have about 1 billion parameters). You can use the smaller checkpoints if you do not have enough GPU space (found on Hugging Face Hub: https://huggingface.co/openai/whisper-small) or decrease the batch size.\n",
    "\n",
    "Note: Whisper requires that input is sampled at 16,000 Hz. FLEURS data is sampled at this rate, so we are good. Also, Whisper does not support all FLEURS languages (the 96 languages are listed in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422ba1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/63 [00:00<?, ?it/s]/data/users/kashrest/miniconda3/envs/asr/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [05:19<00:00,  5.07s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "device = \"cuda:0\" # change this to a gpu if you have access to one, otherwise set to \"cpu\"\n",
    "model_id = \"openai/whisper-large-v2\" # 8630MiB for batch size 1, ~25426MiB for batch size 10\n",
    "processor = WhisperProcessor.from_pretrained(model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id).to(device)\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"hausa\", task=\"transcribe\")\n",
    "\n",
    "predicted_test_transcripts = []\n",
    "\n",
    "batch_size = 10 # decrease if needed\n",
    "\n",
    "for i in tqdm(range(0, len(test_audio), batch_size)):\n",
    "    batch = test_audio[i:i+batch_size] if i+batch_size <= len(test_audio) else test_audio[i:]\n",
    "    input_features = processor(batch, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "    # generate token ids\n",
    "    predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "    # decode token ids to text\n",
    "    predicted_test_transcripts.extend(processor.batch_decode(predicted_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7768e1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(621, 621)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_test_transcripts), len(test_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73753c9b",
   "metadata": {},
   "source": [
    "Let's evaluate the performance of whisper-small on our preprocessed test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "100d70d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "621"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "521fe204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wer': 97.78959507944643, 'cer': 40.52395399540877}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(test_transcripts, predicted_test_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe8e1a",
   "metadata": {},
   "source": [
    "It looks like we have a 97.8% WER and 40.5% CER. Let's create a running table of the performances of different models on our test dataset. \n",
    "\n",
    "| Model | WER % | CER %|\n",
    "|-------|-----|----|\n",
    "|whisper-large-v2| 97.8| 40.5|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a2a508",
   "metadata": {},
   "source": [
    "### Facebook MMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c219b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 621/621 [28:10<00:00,  2.72s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, AutoProcessor\n",
    "\n",
    "model_id = \"facebook/mms-1b-all\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
    "\n",
    "processor.tokenizer.set_target_lang(\"hau\")\n",
    "model.load_adapter(\"hau\")\n",
    "\n",
    "predicted_test_transcripts = []\n",
    "\n",
    "for elem in tqdm(test_audio): # Todo: batch inference?\n",
    "    inputs = processor(elem, sampling_rate=16_000, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "    ids = torch.argmax(outputs, dim=-1)[0]\n",
    "    predicted_test_transcripts.append(processor.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9868c9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wer': 29.279856483854434, 'cer': 7.700116511126236}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(test_transcripts, predicted_test_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4874ff",
   "metadata": {},
   "source": [
    "It looks like we have a 29.3% WER and 7.7% CER. Let's add this result to our table. \n",
    "\n",
    "| Model | WER % | CER %|\n",
    "|-------|-----|----|\n",
    "|whisper-large-v2| 97.8| 40.5|\n",
    "|mms-1b-all|29.3|7.7|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6325a",
   "metadata": {},
   "source": [
    "## Section B: Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aacf6e2",
   "metadata": {},
   "source": [
    "For fine-tuning, we will be using functions from the Hugging Face API for the training loop and model setup. In order to use these functions, we need to wrap the data in a custom PyTorch Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c1e5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio, transcripts, sampling_rate, processor):#feature_extractor, tokenizer):\n",
    "        self.audio = audio\n",
    "        self.transcripts = transcripts\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.processor = processor\n",
    "        \"\"\"self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        input_values = self.processor.feature_extractor(self.audio[idx], sampling_rate=self.sampling_rate).input_features[0]\n",
    "        labels = self.processor.tokenizer(self.transcripts[idx]).input_ids\n",
    "        \n",
    "        item = {}\n",
    "        item[\"input_features\"] = input_values\n",
    "        item[\"labels\"] = labels\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1358da3f",
   "metadata": {},
   "source": [
    "### Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b2a9a4",
   "metadata": {},
   "source": [
    "First, let's import the required Whisper classes and training loop functions from Hugging Face and some other utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd7b8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca4305",
   "metadata": {},
   "source": [
    "Let's fine-tune a Whisper-large V2 model which we will download from Hugging Face, and set up a WhisperProcessor object which takes contains a feature extractor and a tokenizer. The feature extractor extracts features from the input, which in this case is just normalizing the input array. The tokenizer splits the transcripts into tokens based on Whisper's vocabulary. Whisper utilizes byte-level BPE, which is the same tokenizer as GPT-2. If interested, refer to this page: https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cab8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = \"openai/whisper-large-v2\"\n",
    "processor = WhisperProcessor.from_pretrained(model_card, language=\"Hausa\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c83da",
   "metadata": {},
   "source": [
    "Like mentioned before, Whisper takes inputs sampled at 16,000 Hz, and so we will prepare our data using this sampling rate (Fleurs is already sampled at 16 kHz) using the ASRDataset object mentioned before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "806959ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sampling_rate = 16000\n",
    "train_dataset = ASRDataset(train_audio, train_transcripts, model_sampling_rate, processor)\n",
    "val_dataset = ASRDataset(val_audio,  val_transcripts, model_sampling_rate, processor)\n",
    "test_dataset = ASRDataset(test_audio, test_transcripts, model_sampling_rate, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3fab6f",
   "metadata": {},
   "source": [
    "Next, we need a functio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1fdde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "    \n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "metric_wer = evaluate.load(\"wer\")\n",
    "metric_cer = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric_wer.compute(predictions=pred_str, references=label_str)\n",
    "    cer = 100 * metric_cer.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}\n",
    "\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_card)\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=out_dir,  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=500,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2, \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "trainer.train()\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4357c4c",
   "metadata": {},
   "source": [
    "### MMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e91f3a",
   "metadata": {},
   "source": [
    "### XLS-R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ada4a",
   "metadata": {},
   "source": [
    "## Section C: Further Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153feed",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3831d",
   "metadata": {},
   "source": [
    "### Adding more data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

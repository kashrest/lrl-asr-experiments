{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ea45ff",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition (ASR) Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b82bb",
   "metadata": {},
   "source": [
    "## Fine-tune a pretrained, multilingual ASR model on FLEURS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0545f5",
   "metadata": {},
   "source": [
    "In this tutorial, we will be evaluating and improving a multilingual ASR model for a language in the FLEURS dataset. We will focus on **Hausa**, but you can follow along in any language in FLEURS. See the FLEURS dataset paper for a list of supported languages: https://arxiv.org/abs/2205.12446"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945531e",
   "metadata": {},
   "source": [
    "## Before you start: Setting up your coding environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7e5c2",
   "metadata": {},
   "source": [
    "You can run follow along and run the lines of code in this notebook, and also utilize the scripts found in this GitHub respository. Before starting this tutorial, you will need to create a virtual environment for this project so you can download all the required packages without affecting your other projects. We recommend using Anaconda (conda) to create a virtual environment. We have provided an `environment.yml` file that you can use to create a virtual environment named `asr` containing all the required packages. In your terminal run this code:\n",
    "\n",
    "```\n",
    "git clone https://github.com/kashrest/lrl-asr-experiments.git\n",
    "cd lrl-asr-experiments\n",
    "conda env create -f environment.yml \n",
    "conda activate asr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71af9f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbbef6",
   "metadata": {},
   "source": [
    "The first step is to download and prepare the data for the ASR model. Hugging Face has an easy way to download FLEURS data for any supported language, where the split can be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24889296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fleurs (/data/users/kashrest/lrl-asr-experiments/data/fleurs/google___fleurs/ha_ng/2.0.0/af82dbec419a815084fa63ebd5d5a9f24a6e9acdf9887b9e3b8c6bbd64e0b7ac)\n",
      "Found cached dataset fleurs (/data/users/kashrest/lrl-asr-experiments/data/fleurs/google___fleurs/ha_ng/2.0.0/af82dbec419a815084fa63ebd5d5a9f24a6e9acdf9887b9e3b8c6bbd64e0b7ac)\n",
      "Found cached dataset fleurs (/data/users/kashrest/lrl-asr-experiments/data/fleurs/google___fleurs/ha_ng/2.0.0/af82dbec419a815084fa63ebd5d5a9f24a6e9acdf9887b9e3b8c6bbd64e0b7ac)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir_fleurs = \"./data/fleurs\"\n",
    "\n",
    "# create a data directory for caching \n",
    "try:\n",
    "    os.mkdir(cache_dir_fleurs)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# for Hausa, the language code is \"ha_ng\"\n",
    "train_data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"train\", cache_dir=cache_dir_fleurs)\n",
    "val_data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"validation\", cache_dir=cache_dir_fleurs)\n",
    "test_data = load_dataset(\"google/fleurs\", \"ha_ng\", split=\"test\", cache_dir=cache_dir_fleurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945c231",
   "metadata": {},
   "source": [
    "Now, since we are interested in transcribing speech, we want to clean the transcripts by removing special characters that do not have a clear sound (such as ! '). This part may depend on your target application and language. For example for Hausa, many native speakers do not speak English and does not have much code-switching, so we want to also normalize any foreign characters (ç ş) and symbols (% & $)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2158c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_texts(transcriptions):\n",
    "    chars_to_remove_regex = '[\\,\\?\\!\\-\\;\\:\\\"\\“\\%\\‘\\'\\ʻ\\”\\�\\$\\&\\(\\)\\–\\—]'\n",
    "\n",
    "    def _remove_special_characters(transcription):\n",
    "        transcription = transcription.strip()\n",
    "        transcription = transcription.lower()\n",
    "        transcription = re.sub(chars_to_remove_regex, '', transcription)\n",
    "        transcription = re.sub(\"\\[\\]\\{\\}\", '', transcription)\n",
    "        transcription = re.sub(r'[\\\\]', '', transcription)\n",
    "        transcription = re.sub(r'[/]', '', transcription)\n",
    "        transcription = re.sub(u'[¥£°¾½²]', '', transcription)\n",
    "        transcription = re.sub(u'[\\+><]', '', transcription)\n",
    "        return transcription\n",
    "\n",
    "    def _normalize_diacritics(transcription):\n",
    "        a = '[āăáã]'\n",
    "        u = '[ūúü]'\n",
    "        o = '[öõó]' \n",
    "        c = '[ç]'\n",
    "        i = '[í]'\n",
    "        s = '[ş]'\n",
    "        e = '[é]'\n",
    "\n",
    "        transcription = re.sub(a, \"a\", transcription)\n",
    "        transcription = re.sub(u, \"u\", transcription)\n",
    "        transcription = re.sub(o, \"o\", transcription)\n",
    "        transcription = re.sub(c, \"c\", transcription)\n",
    "        transcription = re.sub(i, \"i\", transcription)\n",
    "        transcription = re.sub(s, \"s\", transcription)\n",
    "        transcription = re.sub(e, \"e\", transcription)\n",
    "\n",
    "        return transcription\n",
    "\n",
    "    cleaned_transcriptions = map(_remove_special_characters, transcriptions)\n",
    "    cleaned_transcriptions = list(map(_normalize_diacritics, list(cleaned_transcriptions)))\n",
    "    return cleaned_transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaddb44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
